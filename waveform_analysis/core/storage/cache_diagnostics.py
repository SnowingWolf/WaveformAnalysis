"""
ç¼“å­˜è¯Šæ–­æ¨¡å— - é—®é¢˜è¯Šæ–­ä¸ä¿®å¤å·¥å…·ã€‚

æä¾›ç¼“å­˜å®Œæ•´æ€§æ£€æŸ¥ã€é—®é¢˜è¯Šæ–­å’Œè‡ªåŠ¨ä¿®å¤åŠŸèƒ½ã€‚
"""

from dataclasses import dataclass, field
from enum import Enum
import os
from typing import TYPE_CHECKING, Any, Dict, List, Optional

from ..foundation.utils import exporter
from .cache_analyzer import CacheAnalyzer, CacheEntry

if TYPE_CHECKING:
    pass

export, __all__ = exporter()


@export
class DiagnosticIssueType(Enum):
    """è¯Šæ–­é—®é¢˜ç±»å‹æšä¸¾"""

    VERSION_MISMATCH = "version_mismatch"  # æ’ä»¶ç‰ˆæœ¬ä¸åŒ¹é…
    MISSING_METADATA = "missing_metadata"  # å…ƒæ•°æ®æ–‡ä»¶ç¼ºå¤±
    MISSING_DATA_FILE = "missing_data_file"  # æ•°æ®æ–‡ä»¶ç¼ºå¤±
    SIZE_MISMATCH = "size_mismatch"  # æ–‡ä»¶å¤§å°ä¸åŒ¹é…
    CHECKSUM_FAILED = "checksum_failed"  # æ ¡éªŒå’ŒéªŒè¯å¤±è´¥
    ORPHAN_FILE = "orphan_file"  # å­¤å„¿æ–‡ä»¶ï¼ˆæ— å…ƒæ•°æ®ï¼‰
    STORAGE_VERSION_MISMATCH = "storage_version"  # å­˜å‚¨ç‰ˆæœ¬ä¸åŒ¹é…
    CORRUPTED_METADATA = "corrupted_metadata"  # å…ƒæ•°æ®æŸå
    DTYPE_MISMATCH = "dtype_mismatch"  # æ•°æ®ç±»å‹ä¸åŒ¹é…


@export
@dataclass
class DiagnosticIssue:
    """è¯Šæ–­é—®é¢˜æ•°æ®ç±»

    Attributes:
        issue_type: é—®é¢˜ç±»å‹
        severity: ä¸¥é‡ç¨‹åº¦ ('error', 'warning', 'info')
        run_id: è¿è¡Œæ ‡è¯†ç¬¦
        data_name: æ•°æ®åç§°ï¼ˆå¯ä¸ºç©ºï¼‰
        key: ç¼“å­˜é”®
        description: é—®é¢˜æè¿°
        details: è¯¦ç»†ä¿¡æ¯å­—å…¸
        fixable: æ˜¯å¦å¯è‡ªåŠ¨ä¿®å¤
        fix_action: ä¿®å¤åŠ¨ä½œæè¿°
    """

    issue_type: DiagnosticIssueType
    severity: str  # 'error', 'warning', 'info'
    run_id: str
    data_name: Optional[str]
    key: str
    description: str
    details: Dict[str, Any] = field(default_factory=dict)
    fixable: bool = False
    fix_action: Optional[str] = None

    def __str__(self) -> str:
        severity_icon = {"error": "âŒ", "warning": "âš ï¸", "info": "â„¹ï¸"}.get(self.severity, "?")
        return f"{severity_icon} [{self.issue_type.value}] {self.description}"


@export
class CacheDiagnostics:
    """ç¼“å­˜è¯Šæ–­å·¥å…·

    æä¾›ç¼“å­˜å®Œæ•´æ€§æ£€æŸ¥ã€é—®é¢˜è¯Šæ–­å’Œè‡ªåŠ¨ä¿®å¤åŠŸèƒ½ã€‚

    Features:
        - ç‰ˆæœ¬ä¸åŒ¹é…æ£€æµ‹
        - å­¤å„¿æ–‡ä»¶æ£€æµ‹
        - æ•°æ®å®Œæ•´æ€§éªŒè¯
        - è‡ªåŠ¨ä¿®å¤ï¼ˆæ”¯æŒ dry-runï¼‰

    Examples:
        >>> analyzer = CacheAnalyzer(ctx)
        >>> analyzer.scan()
        >>>
        >>> diag = CacheDiagnostics(analyzer)
        >>> issues = diag.diagnose()
        >>> diag.print_report(issues)
        >>>
        >>> # è‡ªåŠ¨ä¿®å¤ï¼ˆå…ˆ dry-runï¼‰
        >>> result = diag.auto_fix(issues, dry_run=True)
        >>> print(f"å°†ä¿®å¤ {result['would_fix']} ä¸ªé—®é¢˜")
    """

    def __init__(self, analyzer: CacheAnalyzer):
        """åˆå§‹åŒ– CacheDiagnostics

        Args:
            analyzer: CacheAnalyzer å®ä¾‹ï¼ˆéœ€è¦å·²å®Œæˆæ‰«æï¼‰
        """
        self.analyzer = analyzer
        self.ctx = analyzer.ctx

    @property
    def storage(self):
        """è·å–å­˜å‚¨å®ä¾‹"""
        return self.ctx.storage

    def diagnose(
        self,
        run_id: Optional[str] = None,
        check_integrity: bool = True,
        check_orphans: bool = True,
        check_versions: bool = True,
        verbose: bool = True,
    ) -> List[DiagnosticIssue]:
        """æ‰§è¡Œå®Œæ•´è¯Šæ–­

        Args:
            run_id: ä»…è¯Šæ–­æŒ‡å®š runï¼ŒNone åˆ™è¯Šæ–­æ‰€æœ‰
            check_integrity: æ˜¯å¦æ£€æŸ¥æ•°æ®å®Œæ•´æ€§
            check_orphans: æ˜¯å¦æ£€æŸ¥å­¤å„¿æ–‡ä»¶
            check_versions: æ˜¯å¦æ£€æŸ¥ç‰ˆæœ¬ä¸åŒ¹é…
            verbose: æ˜¯å¦æ˜¾ç¤ºè¿›åº¦

        Returns:
            è¯Šæ–­é—®é¢˜åˆ—è¡¨
        """
        issues: List[DiagnosticIssue] = []

        if verbose:
            print("[CacheDiagnostics] å¼€å§‹è¯Šæ–­...")

        # è·å–è¦æ£€æŸ¥çš„æ¡ç›®
        entries = self.analyzer.get_entries(run_id=run_id)

        if verbose:
            print(f"  æ£€æŸ¥ {len(entries)} ä¸ªç¼“å­˜æ¡ç›®...")

        # æ£€æŸ¥æ¯ä¸ªæ¡ç›®
        for entry in entries:
            # ç‰ˆæœ¬æ£€æŸ¥
            if check_versions:
                issue = self.check_version_mismatch(entry)
                if issue:
                    issues.append(issue)

            # å®Œæ•´æ€§æ£€æŸ¥
            if check_integrity:
                integrity_issues = self.check_integrity(entry)
                issues.extend(integrity_issues)

        # æ£€æŸ¥å­¤å„¿æ–‡ä»¶
        if check_orphans:
            if run_id:
                orphan_issues = self.find_orphan_files(run_id)
            else:
                orphan_issues = []
                for r in self.analyzer.get_all_runs():
                    orphan_issues.extend(self.find_orphan_files(r))
            issues.extend(orphan_issues)

        if verbose:
            error_count = sum(1 for i in issues if i.severity == "error")
            warning_count = sum(1 for i in issues if i.severity == "warning")
            info_count = sum(1 for i in issues if i.severity == "info")
            print(
                f"[CacheDiagnostics] è¯Šæ–­å®Œæˆ: {error_count} é”™è¯¯, "
                f"{warning_count} è­¦å‘Š, {info_count} ä¿¡æ¯"
            )

        return issues

    def check_version_mismatch(self, entry: CacheEntry) -> Optional[DiagnosticIssue]:
        """æ£€æŸ¥æ’ä»¶ç‰ˆæœ¬ä¸åŒ¹é…

        Args:
            entry: ç¼“å­˜æ¡ç›®

        Returns:
            DiagnosticIssue æˆ– None
        """
        # è·å–å½“å‰æ’ä»¶ç‰ˆæœ¬
        if not hasattr(self.ctx, "_plugins") or entry.data_name not in self.ctx._plugins:
            return None

        plugin = self.ctx._plugins[entry.data_name]
        current_version = str(getattr(plugin, "version", "unknown"))

        if entry.plugin_version != current_version and entry.plugin_version != "unknown":
            return DiagnosticIssue(
                issue_type=DiagnosticIssueType.VERSION_MISMATCH,
                severity="warning",
                run_id=entry.run_id,
                data_name=entry.data_name,
                key=entry.key,
                description=f"æ’ä»¶ç‰ˆæœ¬ä¸åŒ¹é…: ç¼“å­˜={entry.plugin_version}, å½“å‰={current_version}",
                details={
                    "cached_version": entry.plugin_version,
                    "current_version": current_version,
                },
                fixable=True,
                fix_action="åˆ é™¤ç¼“å­˜ï¼Œé‡æ–°è®¡ç®—",
            )

        return None

    def check_integrity(self, entry: CacheEntry) -> List[DiagnosticIssue]:
        """æ£€æŸ¥å•ä¸ªæ¡ç›®çš„å®Œæ•´æ€§

        Args:
            entry: ç¼“å­˜æ¡ç›®

        Returns:
            è¯Šæ–­é—®é¢˜åˆ—è¡¨
        """
        issues = []

        # æ£€æŸ¥æ•°æ®æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if not os.path.exists(entry.file_path):
            issues.append(
                DiagnosticIssue(
                    issue_type=DiagnosticIssueType.MISSING_DATA_FILE,
                    severity="error",
                    run_id=entry.run_id,
                    data_name=entry.data_name,
                    key=entry.key,
                    description=f"æ•°æ®æ–‡ä»¶ç¼ºå¤±: {entry.file_path}",
                    details={"file_path": entry.file_path},
                    fixable=True,
                    fix_action="åˆ é™¤å­¤ç«‹çš„å…ƒæ•°æ®æ–‡ä»¶",
                )
            )
            return issues

        # æ£€æŸ¥å­˜å‚¨ç‰ˆæœ¬
        storage_version = entry.metadata.get("storage_version")
        if storage_version and hasattr(self.storage, "STORAGE_VERSION"):
            valid_versions = [self.storage.STORAGE_VERSION, "1.0.0"]
            if storage_version not in valid_versions:
                issues.append(
                    DiagnosticIssue(
                        issue_type=DiagnosticIssueType.STORAGE_VERSION_MISMATCH,
                        severity="warning",
                        run_id=entry.run_id,
                        data_name=entry.data_name,
                        key=entry.key,
                        description=f"å­˜å‚¨ç‰ˆæœ¬ä¸åŒ¹é…: {storage_version}",
                        details={
                            "cached_version": storage_version,
                            "expected_versions": valid_versions,
                        },
                        fixable=True,
                        fix_action="åˆ é™¤ç¼“å­˜ï¼Œé‡æ–°è®¡ç®—",
                    )
                )

        # æ£€æŸ¥æ–‡ä»¶å¤§å°ï¼ˆä»…å¯¹éå‹ç¼©æ•°æ®ï¼‰
        if not entry.compressed:
            actual_size = os.path.getsize(entry.file_path)
            expected_size = entry.size_bytes
            if expected_size > 0 and actual_size != expected_size:
                issues.append(
                    DiagnosticIssue(
                        issue_type=DiagnosticIssueType.SIZE_MISMATCH,
                        severity="error",
                        run_id=entry.run_id,
                        data_name=entry.data_name,
                        key=entry.key,
                        description=f"æ–‡ä»¶å¤§å°ä¸åŒ¹é…: å®é™…={actual_size}, æœŸæœ›={expected_size}",
                        details={
                            "actual_size": actual_size,
                            "expected_size": expected_size,
                        },
                        fixable=True,
                        fix_action="åˆ é™¤æŸåçš„ç¼“å­˜",
                    )
                )

        # æ£€æŸ¥æ ¡éªŒå’Œï¼ˆå¦‚æœæœ‰ï¼‰
        if entry.has_checksum and self.storage.verify_on_load:
            checksum_issue = self._verify_checksum(entry)
            if checksum_issue:
                issues.append(checksum_issue)

        return issues

    def _verify_checksum(self, entry: CacheEntry) -> Optional[DiagnosticIssue]:
        """éªŒè¯æ ¡éªŒå’Œ

        Args:
            entry: ç¼“å­˜æ¡ç›®

        Returns:
            DiagnosticIssue æˆ– None
        """
        expected_checksum = entry.metadata.get("checksum")
        algorithm = entry.metadata.get("checksum_algorithm", "xxhash64")

        if not expected_checksum:
            return None

        try:
            from .integrity import get_integrity_checker

            checker = get_integrity_checker()

            if not checker.verify_checksum(entry.file_path, expected_checksum, algorithm):
                return DiagnosticIssue(
                    issue_type=DiagnosticIssueType.CHECKSUM_FAILED,
                    severity="error",
                    run_id=entry.run_id,
                    data_name=entry.data_name,
                    key=entry.key,
                    description="æ ¡éªŒå’ŒéªŒè¯å¤±è´¥ï¼Œæ•°æ®å¯èƒ½å·²æŸå",
                    details={
                        "expected_checksum": expected_checksum,
                        "algorithm": algorithm,
                    },
                    fixable=True,
                    fix_action="åˆ é™¤æŸåçš„ç¼“å­˜",
                )
        except Exception as e:
            return DiagnosticIssue(
                issue_type=DiagnosticIssueType.CHECKSUM_FAILED,
                severity="warning",
                run_id=entry.run_id,
                data_name=entry.data_name,
                key=entry.key,
                description=f"æ ¡éªŒå’ŒéªŒè¯å‡ºé”™: {e}",
                details={"error": str(e)},
                fixable=False,
                fix_action=None,
            )

        return None

    def find_orphan_files(self, run_id: str) -> List[DiagnosticIssue]:
        """æŸ¥æ‰¾å­¤å„¿æ–‡ä»¶ï¼ˆæœ‰æ•°æ®æ–‡ä»¶ä½†æ— å…ƒæ•°æ®ï¼‰

        Args:
            run_id: è¿è¡Œæ ‡è¯†ç¬¦

        Returns:
            å­¤å„¿æ–‡ä»¶é—®é¢˜åˆ—è¡¨
        """
        issues = []

        # è·å–æ•°æ®ç›®å½•
        if hasattr(self.storage, "get_run_data_dir"):
            data_dir = self.storage.get_run_data_dir(run_id)
        elif hasattr(self.storage, "base_dir"):
            if hasattr(self.storage, "use_run_subdirs") and self.storage.use_run_subdirs:
                data_dir = os.path.join(self.storage.work_dir, run_id, self.storage.data_subdir)
            else:
                data_dir = self.storage.base_dir
        else:
            return issues

        if not os.path.exists(data_dir):
            return issues

        # æ‰«ææ•°æ®æ–‡ä»¶
        data_extensions = {".bin", ".blosc2", ".lz4", ".zst", ".gz", ".parquet"}

        for filename in os.listdir(data_dir):
            # æ£€æŸ¥æ˜¯å¦æ˜¯æ•°æ®æ–‡ä»¶
            name, ext = os.path.splitext(filename)
            if ext not in data_extensions:
                continue

            # å¤„ç†å‹ç¼©æ–‡ä»¶çš„åŒæ‰©å±•åï¼ˆå¦‚ .bin.blosc2ï¼‰
            if ext in {".blosc2", ".lz4", ".zst", ".gz"}:
                base_name, _ = os.path.splitext(name)
                key = base_name
            else:
                key = name

            # æ£€æŸ¥æ˜¯å¦æœ‰å¯¹åº”çš„å…ƒæ•°æ®
            meta_path = os.path.join(data_dir, f"{key}.json")
            if not os.path.exists(meta_path):
                file_path = os.path.join(data_dir, filename)
                file_size = os.path.getsize(file_path)

                issues.append(
                    DiagnosticIssue(
                        issue_type=DiagnosticIssueType.ORPHAN_FILE,
                        severity="warning",
                        run_id=run_id,
                        data_name=None,
                        key=key,
                        description=f"å­¤å„¿æ–‡ä»¶ï¼ˆæ— å…ƒæ•°æ®ï¼‰: {filename}",
                        details={
                            "file_path": file_path,
                            "file_size": file_size,
                            "file_size_human": self._format_size(file_size),
                        },
                        fixable=True,
                        fix_action="åˆ é™¤å­¤å„¿æ–‡ä»¶",
                    )
                )

        return issues

    def print_report(
        self, issues: List[DiagnosticIssue], group_by: str = "severity", show_fixable: bool = True
    ):
        """æ‰“å°è¯Šæ–­æŠ¥å‘Š

        Args:
            issues: è¯Šæ–­é—®é¢˜åˆ—è¡¨
            group_by: åˆ†ç»„æ–¹å¼ ('severity', 'type', 'run_id')
            show_fixable: æ˜¯å¦æ˜¾ç¤ºå¯ä¿®å¤ä¿¡æ¯
        """
        if not issues:
            print("\nâœ“ ç¼“å­˜è¯Šæ–­å®Œæˆï¼Œæœªå‘ç°é—®é¢˜")
            return

        print("\n" + "=" * 70)
        print("ç¼“å­˜è¯Šæ–­æŠ¥å‘Š")
        print("=" * 70)

        # ç»Ÿè®¡
        error_count = sum(1 for i in issues if i.severity == "error")
        warning_count = sum(1 for i in issues if i.severity == "warning")
        info_count = sum(1 for i in issues if i.severity == "info")
        fixable_count = sum(1 for i in issues if i.fixable)

        print(f"\næ€»è®¡: {len(issues)} ä¸ªé—®é¢˜")
        print(f"  âŒ é”™è¯¯: {error_count}")
        print(f"  âš ï¸  è­¦å‘Š: {warning_count}")
        print(f"  â„¹ï¸  ä¿¡æ¯: {info_count}")
        if show_fixable:
            print(f"  ğŸ”§ å¯è‡ªåŠ¨ä¿®å¤: {fixable_count}")

        print("\n" + "-" * 70)

        # åˆ†ç»„æ˜¾ç¤º
        if group_by == "severity":
            for severity in ["error", "warning", "info"]:
                severity_issues = [i for i in issues if i.severity == severity]
                if severity_issues:
                    icon = {"error": "âŒ", "warning": "âš ï¸", "info": "â„¹ï¸"}[severity]
                    print(f"\n{icon} {severity.upper()} ({len(severity_issues)})")
                    for issue in severity_issues:
                        self._print_issue(issue, show_fixable)

        elif group_by == "type":
            types = {i.issue_type for i in issues}
            for issue_type in sorted(types, key=lambda t: t.value):
                type_issues = [i for i in issues if i.issue_type == issue_type]
                print(f"\n[{issue_type.value}] ({len(type_issues)})")
                for issue in type_issues:
                    self._print_issue(issue, show_fixable)

        elif group_by == "run_id":
            runs = {i.run_id for i in issues}
            for run_id in sorted(runs):
                run_issues = [i for i in issues if i.run_id == run_id]
                print(f"\n{run_id} ({len(run_issues)})")
                for issue in run_issues:
                    self._print_issue(issue, show_fixable)

        print("\n" + "=" * 70)

    def _print_issue(self, issue: DiagnosticIssue, show_fixable: bool):
        """æ‰“å°å•ä¸ªé—®é¢˜"""
        print(f"    â€¢ {issue.description}")
        print(f"      Key: {issue.key}")
        if show_fixable and issue.fixable:
            print(f"      ğŸ”§ ä¿®å¤æ–¹æ³•: {issue.fix_action}")

    def auto_fix(
        self,
        issues: List[DiagnosticIssue],
        dry_run: bool = True,
        fix_types: Optional[List[DiagnosticIssueType]] = None,
    ) -> Dict[str, Any]:
        """è‡ªåŠ¨ä¿®å¤é—®é¢˜

        Args:
            issues: è¦ä¿®å¤çš„é—®é¢˜åˆ—è¡¨
            dry_run: å¦‚æœä¸º Trueï¼ŒåªæŠ¥å‘Šå°†è¦æ‰§è¡Œçš„æ“ä½œ
            fix_types: è¦ä¿®å¤çš„é—®é¢˜ç±»å‹åˆ—è¡¨ï¼ŒNone åˆ™ä¿®å¤æ‰€æœ‰å¯ä¿®å¤çš„é—®é¢˜

        Returns:
            ä¿®å¤ç»“æœç»Ÿè®¡
        """
        result = {
            "dry_run": dry_run,
            "total": len(issues),
            "fixable": 0,
            "fixed": 0,
            "skipped": 0,
            "failed": 0,
            "details": [],
        }

        # è¿‡æ»¤å¯ä¿®å¤çš„é—®é¢˜
        fixable_issues = [i for i in issues if i.fixable]
        if fix_types:
            fixable_issues = [i for i in fixable_issues if i.issue_type in fix_types]

        result["fixable"] = len(fixable_issues)

        if dry_run:
            print(f"\n[Dry-Run] å°†ä¿®å¤ {len(fixable_issues)} ä¸ªé—®é¢˜:")

        for issue in fixable_issues:
            success = self._fix_issue(issue, dry_run)
            if success:
                result["fixed"] += 1
                result["details"].append(
                    {
                        "key": issue.key,
                        "type": issue.issue_type.value,
                        "action": issue.fix_action,
                        "status": "fixed" if not dry_run else "would_fix",
                    }
                )
            else:
                result["failed"] += 1
                result["details"].append(
                    {
                        "key": issue.key,
                        "type": issue.issue_type.value,
                        "action": issue.fix_action,
                        "status": "failed",
                    }
                )

        result["skipped"] = result["total"] - result["fixable"]

        if dry_run:
            print("\n[Dry-Run] å®Œæˆã€‚å®é™…æ‰§è¡Œè¯·è®¾ç½® dry_run=False")
        else:
            print(f"\n[ä¿®å¤å®Œæˆ] å·²ä¿®å¤: {result['fixed']}, å¤±è´¥: {result['failed']}")

        return result

    def _fix_issue(self, issue: DiagnosticIssue, dry_run: bool) -> bool:
        """ä¿®å¤å•ä¸ªé—®é¢˜

        Args:
            issue: è¦ä¿®å¤çš„é—®é¢˜
            dry_run: æ˜¯å¦ä¸ºæ¼”ç»ƒæ¨¡å¼

        Returns:
            æ˜¯å¦æˆåŠŸ
        """
        try:
            if issue.issue_type in {
                DiagnosticIssueType.VERSION_MISMATCH,
                DiagnosticIssueType.STORAGE_VERSION_MISMATCH,
                DiagnosticIssueType.SIZE_MISMATCH,
                DiagnosticIssueType.CHECKSUM_FAILED,
                DiagnosticIssueType.MISSING_DATA_FILE,
            }:
                # åˆ é™¤ç¼“å­˜
                if dry_run:
                    print(f"  [would delete] {issue.key}")
                else:
                    self.storage.delete(issue.key, issue.run_id)
                    print(f"  [deleted] {issue.key}")
                return True

            elif issue.issue_type == DiagnosticIssueType.ORPHAN_FILE:
                # åˆ é™¤å­¤å„¿æ–‡ä»¶
                file_path = issue.details.get("file_path")
                if file_path and os.path.exists(file_path):
                    if dry_run:
                        print(f"  [would delete] {file_path}")
                    else:
                        os.remove(file_path)
                        print(f"  [deleted] {file_path}")
                    return True

            return False

        except Exception as e:
            print(f"  [error] ä¿®å¤ {issue.key} å¤±è´¥: {e}")
            return False

    @staticmethod
    def _format_size(size_bytes: int) -> str:
        """æ ¼å¼åŒ–å¤§å°"""
        if size_bytes < 1024:
            return f"{size_bytes} B"
        elif size_bytes < 1024 * 1024:
            return f"{size_bytes / 1024:.1f} KB"
        elif size_bytes < 1024 * 1024 * 1024:
            return f"{size_bytes / (1024 * 1024):.1f} MB"
        else:
            return f"{size_bytes / (1024 * 1024 * 1024):.2f} GB"
