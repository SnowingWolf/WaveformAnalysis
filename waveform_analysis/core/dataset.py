# -*- coding: utf-8 -*-
"""
Dataset æ¨¡å— - é¢å‘ç”¨æˆ·çš„é«˜å±‚ API å°è£…ã€‚

**å·²å¼ƒç”¨**: æœ¬æ¨¡å—ä¸­çš„ `WaveformDataset` ç±»å·²è¢«å¼ƒç”¨ï¼Œå°†åœ¨ä¸‹ä¸€ä¸ªä¸»ç‰ˆæœ¬ä¸­ç§»é™¤ã€‚
è¯·ä½¿ç”¨ `Context` å’Œæ’ä»¶ç³»ç»Ÿæ›¿ä»£ã€‚

è¿ç§»æŒ‡å—:
    æ—§ä»£ç :
        from waveform_analysis import WaveformDataset
        ds = WaveformDataset(run_name="run_001", n_channels=2)
        ds.load_raw_data().extract_waveforms()...

    æ–°ä»£ç :
        from waveform_analysis.core import Context
        from waveform_analysis.core.plugins.builtin import standard_plugins
        ctx = Context()
        ctx.register(standard_plugins)
        ctx.set_config({'n_channels': 2, 'data_root': 'DAQ'})
        peaks = ctx.get_data('run_001', 'peaks')
"""

# 1. Standard library imports
import os
import warnings
from typing import Any, Dict, List, Optional, Tuple, Union

# 2. Third-party imports
import numpy as np
import pandas as pd

# 3. Local imports (ä½¿ç”¨ç›¸å¯¹å¯¼å…¥)
from .context import Context
from .foundation.constants import FeatureDefaults
from .foundation.mixins import CacheMixin, StepMixin, chainable_step
from .plugins import builtin as standard_plugins


class WaveformDataset(CacheMixin, StepMixin):
    """
    ç»Ÿä¸€çš„æ³¢å½¢æ•°æ®é›†å®¹å™¨ï¼Œå°è£…æ•´ä¸ªæ•°æ®å¤„ç†æµç¨‹ã€‚
    æ”¯æŒé“¾å¼è°ƒç”¨ï¼Œç®€åŒ–æ•°æ®åŠ è½½ã€é¢„å¤„ç†å’Œåˆ†æã€‚
    
    .. deprecated:: 0.2.0
        `WaveformDataset` å·²è¢«å¼ƒç”¨ï¼Œå°†åœ¨ä¸‹ä¸€ä¸ªä¸»ç‰ˆæœ¬ä¸­ç§»é™¤ã€‚
        è¯·ä½¿ç”¨ `Context` å’Œæ’ä»¶ç³»ç»Ÿæ›¿ä»£ã€‚
        
        è¿ç§»ç¤ºä¾‹:
            æ—§ä»£ç :
                ds = WaveformDataset(run_name="run_001", n_channels=2)
                ds.load_raw_data().extract_waveforms()...
            
            æ–°ä»£ç :
                ctx = Context()
                ctx.register(standard_plugins)
                ctx.set_config({'n_channels': 2, 'data_root': 'DAQ'})
                peaks = ctx.get_data('run_001', 'peaks')
    
    ä½¿ç”¨ç¤ºä¾‹ï¼ˆå·²å¼ƒç”¨ï¼‰ï¼š
        dataset = WaveformDataset(run_name="50V_OV_circulation_20thr", n_channels=2)
        dataset.load_raw_data().extract_waveforms().structure_waveforms()\\
               .build_waveform_features().build_dataframe().group_events()\\
               .pair_events().save_results()
        
        df_paired = dataset.get_paired_events()
        summary = dataset.summary()
    """

    def __init__(
        self,
        run_name: str = "50V_OV_circulation_20thr",
        n_channels: int = 2,
        start_channel_slice: int = 6,
        data_root: str = "DAQ",
        load_waveforms: bool = True,
        use_daq_scan: bool = False,
        daq_root: Optional[str] = None,
        daq_report: Optional[str] = None,
        cache_waveforms: bool = True,
        cache_dir: Optional[str] = None,
        **kwargs,
    ):
        """
        åˆå§‹åŒ–æ•°æ®é›†ã€‚

        .. deprecated:: 0.2.0
            `WaveformDataset` å·²è¢«å¼ƒç”¨ï¼Œå°†åœ¨ä¸‹ä¸€ä¸ªä¸»ç‰ˆæœ¬ä¸­ç§»é™¤ã€‚
            è¯·ä½¿ç”¨ `Context` å’Œæ’ä»¶ç³»ç»Ÿæ›¿ä»£ã€‚

        å‚æ•°:
            run_name: æ•°æ®é›†æ ‡è¯†ç¬¦
            n_channels: è¦å¤„ç†çš„é€šé“æ•°
            start_channel_slice: å¼€å§‹é€šé“ç´¢å¼•ï¼ˆé€šå¸¸ä¸º 6 è¡¨ç¤º CH6/CH7ï¼‰
            data_root: æ•°æ®æ ¹ç›®å½•
            load_waveforms: æ˜¯å¦åŠ è½½åŸå§‹æ³¢å½¢æ•°æ®ï¼ˆé»˜è®¤ Trueï¼‰
                           - True: åŠ è½½æ‰€æœ‰æ³¢å½¢ï¼Œæ”¯æŒ get_waveform_at()
                           - False: ä»…åŠ è½½ç‰¹å¾ï¼ˆå³°å€¼ã€ç”µè·ç­‰ï¼‰ï¼ŒèŠ‚çœå†…å­˜ (70-80% å†…å­˜èŠ‚çœ)
            cache_waveforms: æ˜¯å¦ç¼“å­˜æå–åçš„æ³¢å½¢æ•°æ®åˆ°ç£ç›˜ï¼ˆé»˜è®¤ Trueï¼‰
            cache_dir: ç¼“å­˜ç›®å½•ï¼Œé»˜è®¤ä¸º outputs/_cache
        """
        warnings.warn(
            "WaveformDataset å·²è¢«å¼ƒç”¨ï¼Œå°†åœ¨ä¸‹ä¸€ä¸ªä¸»ç‰ˆæœ¬ä¸­ç§»é™¤ã€‚"
            "è¯·ä½¿ç”¨ Context å’Œæ’ä»¶ç³»ç»Ÿæ›¿ä»£ã€‚"
            "è¿ç§»æŒ‡å—: ä½¿ç”¨ ctx = Context() å’Œ ctx.register() æ³¨å†Œæ’ä»¶ï¼Œ"
            "ç„¶åä½¿ç”¨ ctx.get_data(run_id, data_name) è·å–æ•°æ®ã€‚"
            "æ›´å¤šä¿¡æ¯è¯·å‚è€ƒæ–‡æ¡£: docs/user-guide/QUICKSTART_GUIDE.md",
            DeprecationWarning,
            stacklevel=2,
        )
        CacheMixin.__init__(self)
        StepMixin.__init__(self)

        # å…¼å®¹æ—§çš„ char å‚æ•°
        if "char" in kwargs:
            run_name = kwargs.pop("char")

        self.run_name = run_name
        self.char = run_name  # ä¿æŒ char å±æ€§ä»¥å…¼å®¹æ—§ä»£ç 
        self.n_channels = n_channels
        self.start_channel_slice = start_channel_slice
        self.data_root = data_root
        self.data_dir = os.path.join(data_root, run_name)
        self.load_waveforms = load_waveforms
        self.cache_waveforms = cache_waveforms
        self.cache_dir = cache_dir or os.path.join("outputs", "_cache")

        # åˆå§‹åŒ–æ’ä»¶å¼ Context
        self.ctx = Context(storage_dir=self.cache_dir)
        self.ctx.register(standard_plugins)
        self.ctx.set_config({
            "n_channels": n_channels,
            "start_channel_slice": start_channel_slice,
            "data_root": data_root,
            "peaks_range": FeatureDefaults.PEAK_RANGE,
            "charge_range": FeatureDefaults.CHARGE_RANGE,
            "time_window_ns": FeatureDefaults.TIME_WINDOW_NS,
        })

        # DAQ é›†æˆé€‰é¡¹
        self.use_daq_scan = use_daq_scan
        self.daq_root = daq_root or data_root
        self.daq_report = daq_report
        self.daq_run = None
        self.daq_info = None

        # æ•°æ®å®¹å™¨
        self.raw_files: List[List[str]] = []
        self.waveforms: List[np.ndarray] = []
        self.st_waveforms: List[np.ndarray] = []

        # ç‰¹å¾å’Œç»“æœ
        self.peaks: List[np.ndarray] = []
        self.charges: List[np.ndarray] = []
        self.df: Optional[pd.DataFrame] = None
        self.df_events: Optional[pd.DataFrame] = None
        self.df_paired: Optional[pd.DataFrame] = None

        # ç¼“å­˜ï¼štimestamp -> index
        self._timestamp_index: List[Dict[int, int]] = []

        # å‚æ•°ç¼“å­˜
        self.peaks_range: Tuple[int, int] = FeatureDefaults.PEAK_RANGE
        self.charge_range: Tuple[int, int] = FeatureDefaults.CHARGE_RANGE
        self.time_window_ns: float = FeatureDefaults.TIME_WINDOW_NS

        self._validate_data_dir()

    # -------- å†…éƒ¨å·¥å…· --------

    def _build_timestamp_index(self):
        """ä¸ºæ¯ä¸ªé€šé“æ„å»º timestamp åˆ°è¡Œå·çš„æ˜ å°„ï¼ŒåŠ é€Ÿæ³¢å½¢æŸ¥æ‰¾ã€‚"""
        self._timestamp_index = []
        for ch_arr in self.st_waveforms:
            if len(ch_arr) == 0:
                self._timestamp_index.append({})
                continue
            ts = ch_arr["timestamp"].astype(np.int64)
            self._timestamp_index.append({int(t): int(i) for i, t in enumerate(ts)})

    # -------- é“¾å¼æ­¥éª¤è£…é¥°å™¨ï¼ˆé”™è¯¯å®¹å¿ï¼‰ --------
    def _ensure_timestamp_index(self):
        if not self._timestamp_index:
            self._build_timestamp_index()

    # -------- DAQ é›†æˆæ–¹æ³• --------
    def check_daq_status(self):
        """å°è¯•ä½¿ç”¨ DAQAnalyzerï¼ˆæˆ– JSON æŠ¥å‘Šï¼‰è·å–å½“å‰è¿è¡Œçš„å…ƒä¿¡æ¯ã€‚

        è¿”å›: dict æˆ– Noneï¼ˆè‹¥æ²¡æœ‰æ‰¾åˆ°ï¼‰
        """
        # 1) è‹¥æŒ‡å®šäº† JSON æŠ¥å‘Šè·¯å¾„ï¼Œä¼˜å…ˆä½¿ç”¨
        if self.daq_report:
            return self._load_daq_report(self.daq_report)

        # 2) å¦åˆ™ä½¿ç”¨ DAQAnalyzer è¿›è¡Œæ‰«æï¼ˆæ‡’å¯¼å…¥ä»¥é¿å…é¢å¤–ä¾èµ–å¯åŠ¨å¼€é”€ï¼‰
        try:
            from waveform_analysis.utils.daq import DAQAnalyzer

            analyzer = DAQAnalyzer(self.daq_root)
            analyzer.scan_all_runs()
            run = analyzer.get_run(self.run_name)
            if run is None:
                # æœªåœ¨æ‰«æç»“æœä¸­æ‰¾åˆ°
                self.daq_run = None
                self.daq_info = None
                return None

            # ä¿å­˜ DAQRun å¯¹è±¡ä»¥ä¾¿åç»­ä½¿ç”¨
            self.daq_run = run
            self.daq_info = {
                "run_name": run.run_name,
                "path": run.run_path,
                "channels": sorted(run.channels),
            }
            # ä¹Ÿè¿”å› JSON é£æ ¼çš„ channel_details
            channel_details = {}
            for ch in sorted(run.channel_files.keys()):
                files = run.channel_files[ch]
                channel_details[str(ch)] = {
                    "file_count": len(files),
                    "files": [
                        {"filename": f["filename"], "index": f.get("index", 0)}
                        for f in sorted(files, key=lambda x: x.get("index", 0))
                    ],
                }
            self.daq_info["channel_details"] = channel_details
            return self.daq_info
        except Exception:
            # è§£æå¤±è´¥åˆ™æ¸…ç©ºçŠ¶æ€å¹¶è¿”å› None
            self.daq_run = None
            self.daq_info = None
            return None

    def _load_daq_report(self, report_path: str):
        """ä» JSON æŠ¥å‘ŠåŠ è½½è¿è¡Œä¿¡æ¯ï¼ˆreport_path å¯ä»¥æ˜¯æ–‡ä»¶è·¯å¾„æˆ–å·²ç»åŠ è½½çš„ dictï¼‰ã€‚"""
        if isinstance(report_path, dict):
            report = report_path
        else:
            if not os.path.exists(report_path):
                return None
            try:
                import json

                report = json.loads(open(report_path, "r", encoding="utf-8").read())
            except Exception:
                return None

        # ä»æŠ¥å‘Šä¸­æŸ¥æ‰¾åŒ¹é…çš„ run
        for r in report.get("runs", []):
            if r.get("run_name") == self.run_name:
                self.daq_info = r
                self.daq_run = None
                return r
        return None

    # -------- é“¾å¼æ­¥éª¤ç›¸å…³å·¥å…· --------
    def clear_cache(
        self, 
        step_name: Optional[str] = None,
        clear_memory: bool = True,
        clear_disk: bool = True
    ) -> int:
        """
        æ¸…ç†ç¼“å­˜ã€‚
        
        å‚æ•°:
            step_name: æ­¥éª¤åç§°ï¼ˆå¦‚ "st_waveforms", "df"ï¼‰ï¼Œå¦‚æœä¸º None åˆ™æ¸…ç†æ‰€æœ‰æ­¥éª¤
            clear_memory: æ˜¯å¦æ¸…ç†å†…å­˜ç¼“å­˜
            clear_disk: æ˜¯å¦æ¸…ç†ç£ç›˜ç¼“å­˜
        
        è¿”å›:
            æ¸…ç†çš„ç¼“å­˜é¡¹æ•°é‡
        
        ç¤ºä¾‹:
            >>> ds = WaveformDataset(...)
            >>> # æ¸…ç†å•ä¸ªæ­¥éª¤çš„ç¼“å­˜
            >>> ds.clear_cache("st_waveforms")
            >>> # æ¸…ç†æ‰€æœ‰ç¼“å­˜
            >>> ds.clear_cache()
            >>> # åªæ¸…ç†å†…å­˜ç¼“å­˜
            >>> ds.clear_cache("df", clear_disk=False)
        """
        return self.ctx.clear_cache_for(
            self.run_name, 
            step_name, 
            clear_memory=clear_memory,
            clear_disk=clear_disk
        )

    def _validate_data_dir(self):
        """éªŒè¯æ•°æ®ç›®å½•æ˜¯å¦å­˜åœ¨ã€‚è‹¥å¯ç”¨äº† DAQ æ‰«æï¼Œå¯åœ¨ç›®å½•ç¼ºå¤±æ—¶å°è¯•ä» DAQ æ‰«æè·å–è¿è¡Œä¿¡æ¯ã€‚"""
        if not os.path.exists(self.data_dir):
            if not self.use_daq_scan:
                raise FileNotFoundError(f"æ•°æ®ç›®å½•ä¸å­˜åœ¨: {self.data_dir}")

            # å°è¯•ä½¿ç”¨ DAQ æ‰«æ/æŠ¥å‘Šè·å–è¿è¡Œä¿¡æ¯
            try:
                info = self.check_daq_status()
                if info is None:
                    raise FileNotFoundError(
                        f"æ•°æ®ç›®å½• {self.data_dir} ä¸å­˜åœ¨ï¼ŒDAQ æ‰«æä¹Ÿæœªå‘ç°è¿è¡Œ {self.run_name} (daq_root={self.daq_root})"
                    )
            except Exception:
                raise FileNotFoundError(f"æ•°æ®ç›®å½•ä¸å­˜åœ¨: {self.data_dir} ä¸”æ— æ³•ä» DAQ æ‰«æè·å–ä¿¡æ¯")

    @chainable_step
    def load_raw_data(self, verbose: bool = True) -> "WaveformDataset":
        """
        åŠ è½½åŸå§‹ CSV æ–‡ä»¶ã€‚
        """
        self.raw_files = self.ctx.get_data(self.run_name, "raw_files")

        if verbose:
            print(f"[{self.run_name}] åŠ è½½ {len(self.raw_files)} ä¸ªé€šé“çš„åŸå§‹æ–‡ä»¶")
        return self

    @chainable_step
    def extract_waveforms(self, verbose: bool = True, **kwargs) -> "WaveformDataset":
        """
        ä»åŸå§‹æ–‡ä»¶ä¸­æå–æ³¢å½¢æ•°æ®ã€‚
        """
        self.waveforms = self.ctx.get_data(self.run_name, "waveforms", **kwargs)

        if verbose:
            for ch, wf in enumerate(self.waveforms):
                if wf.size > 0:
                    print(f"  CH{self.start_channel_slice + ch}: {wf.shape}")
        return self

    def clear_waveforms(self) -> None:
        """é‡Šæ”¾æ³¢å½¢ç›¸å…³çš„å¤§å—å†…å­˜ï¼ˆwaveforms ä¸ st_waveformsï¼‰ã€‚"""
        self.waveforms = []
        self.st_waveforms = []
        self._timestamp_index = []

    @chainable_step
    def structure_waveforms(self, verbose: bool = True) -> "WaveformDataset":
        """
        å°†æ³¢å½¢æ•°æ®è½¬æ¢ä¸ºç»“æ„åŒ– numpy æ•°ç»„ã€‚
        """
        if not self.load_waveforms:
            return self

        self.st_waveforms = self.ctx.get_data(self.run_name, "st_waveforms")
        self._build_timestamp_index()

        if verbose:
            n_events = [len(st_ch) for st_ch in self.st_waveforms]
            print(f"ç»“æ„åŒ–æ³¢å½¢å®Œæˆï¼Œå„é€šé“äº‹ä»¶æ•°: {n_events}")
        return self

    @chainable_step
    def build_waveform_features(
        self,
        peaks_range: Optional[Tuple[int, int]] = None,
        charge_range: Optional[Tuple[int, int]] = None,
        verbose: bool = True,
    ) -> "WaveformDataset":
        """
        è®¡ç®—æ³¢å½¢ç‰¹å¾ï¼ˆpeaks å’Œ chargesï¼‰ã€‚
        """
        if peaks_range:
            self.peaks_range = peaks_range
        if charge_range:
            self.charge_range = charge_range

        self.ctx.set_config({
            "peaks_range": self.peaks_range,
            "charge_range": self.charge_range,
        })

        self.peaks = self.ctx.get_data(self.run_name, "peaks")
        self.charges = self.ctx.get_data(self.run_name, "charges")

        if verbose:
            print(f"ç‰¹å¾è®¡ç®—å®Œæˆ: {len(self.peaks)} é€šé“")
        return self

    @chainable_step
    def build_dataframe(self, verbose: bool = True) -> "WaveformDataset":
        """
        æ„å»ºæ³¢å½¢ DataFrameã€‚
        """
        self.df = self.ctx.get_data(self.run_name, "df")

        if verbose:
            print(f"æ„å»º DataFrame å®Œæˆï¼Œäº‹ä»¶æ€»æ•°: {len(self.df)}")
        return self

    @chainable_step
    def group_events(
        self,
        time_window_ns: Optional[float] = None,
        use_numba: bool = True,
        n_processes: Optional[int] = None,
        verbose: bool = True,
    ) -> "WaveformDataset":
        """
        æŒ‰æ—¶é—´çª—å£èšç±»å¤šé€šé“äº‹ä»¶ã€‚
        
        å‚æ•°:
            time_window_ns: æ—¶é—´çª—å£ï¼ˆçº³ç§’ï¼‰
            use_numba: æ˜¯å¦ä½¿ç”¨numbaåŠ é€Ÿï¼ˆé»˜è®¤Trueï¼‰
            n_processes: å¤šè¿›ç¨‹æ•°é‡ï¼ˆNone=å•è¿›ç¨‹ï¼Œ>1=å¤šè¿›ç¨‹ï¼‰
            verbose: æ˜¯å¦æ‰“å°æ—¥å¿—
        """
        tw = time_window_ns or self.time_window_ns
        self.ctx.set_config({
            "time_window_ns": tw,
            "use_numba": use_numba,
            "n_processes": n_processes,
        })
        self.df_events = self.ctx.get_data(self.run_name, "df_events")

        if time_window_ns:
            self.time_window_ns = time_window_ns

        if verbose:
            print(f"èšç±»å®Œæˆï¼Œäº‹ä»¶ç»„æ•°: {len(self.df_events)}")
            if n_processes and n_processes > 1:
                print(f"  ä½¿ç”¨ {n_processes} ä¸ªè¿›ç¨‹å¹¶è¡Œå¤„ç†")
            elif use_numba:
                try:
                    import numba
                    print("  ä½¿ç”¨ numba åŠ é€Ÿ")
                except ImportError:
                    pass
        return self

    @chainable_step
    def pair_events(
        self, n_channels: Optional[int] = None, start_channel_slice: Optional[int] = None, verbose: bool = True
    ) -> "WaveformDataset":
        """
        ç­›é€‰æˆå¯¹çš„ N é€šé“äº‹ä»¶ã€‚
        """
        n_ch = n_channels or self.n_channels
        start_ch = start_channel_slice or self.start_channel_slice

        self.ctx.set_config({"n_channels": n_ch, "start_channel_slice": start_ch})
        self.df_paired = self.ctx.get_data(self.run_name, "df_paired")

        if verbose:
            print(f"é…å¯¹å®Œæˆï¼ŒæˆåŠŸé…å¯¹çš„äº‹ä»¶æ•°: {len(self.df_paired)}")
        return self

    def get_raw_events(self) -> Optional[pd.DataFrame]:
        """è·å–åŸå§‹äº‹ä»¶ DataFrameï¼ˆæœªåˆ†ç»„ï¼‰ã€‚"""
        return self.df

    def get_grouped_events(self) -> Optional[pd.DataFrame]:
        """è·å–åˆ†ç»„åçš„äº‹ä»¶ DataFrameã€‚"""
        return self.df_events

    def get_paired_events(self) -> Optional[pd.DataFrame]:
        """è·å–é…å¯¹çš„äº‹ä»¶ DataFrameã€‚"""
        return self.df_paired

    def get_waveform_at(self, event_idx: int, channel: int = 0) -> Optional[Tuple[np.ndarray, float]]:
        """
        è·å–æŒ‡å®šäº‹ä»¶å’Œé€šé“çš„åŸå§‹æ³¢å½¢åŠå…¶ baselineã€‚

        å‚æ•°:
            event_idx: df_paired ä¸­çš„äº‹ä»¶ç´¢å¼•
            channel: é€šé“ç´¢å¼•ï¼ˆç›¸å¯¹äº start_channel_sliceï¼‰

        è¿”å›: (æ³¢å½¢æ•°ç»„, baseline) æˆ– None
        """
        if not self.load_waveforms:
            print("âš ï¸  æ³¢å½¢æ•°æ®æœªåŠ è½½ï¼ˆload_waveforms=Falseï¼‰")
            return None

        if self.df_paired is None or event_idx >= len(self.df_paired):
            return None

        event = self.df_paired.iloc[event_idx]
        ts = event["timestamps"][channel]

        # ä½¿ç”¨é¢„æ„å»ºç´¢å¼•åŠ é€ŸæŸ¥æ‰¾
        self._ensure_timestamp_index()
        idx_map = self._timestamp_index[channel] if channel < len(self._timestamp_index) else {}
        idx = idx_map.get(int(ts), -1)
        if idx == -1:
            return None

        try:
            wave = self.st_waveforms[channel][idx]["wave"]
            baseline = self.st_waveforms[channel][idx]["baseline"]
            return wave, baseline
        except (IndexError, ValueError):
            return None

    def save_results(self, output_dir: str = "outputs", verbose: bool = True) -> "WaveformDataset":
        """
        ä¿å­˜å¤„ç†ç»“æœï¼ˆCSV å’Œ Parquet æ ¼å¼ï¼‰ã€‚

        å‚æ•°:
            output_dir: è¾“å‡ºç›®å½•
            verbose: æ˜¯å¦æ‰“å°æ—¥å¿—

        è¿”å›: selfï¼ˆä¾¿äºé“¾å¼è°ƒç”¨ï¼‰
        """
        os.makedirs(output_dir, exist_ok=True)

        if self.df_paired is not None and len(self.df_paired) > 0:
            csv_path = os.path.join(output_dir, f"{self.run_name}_paired.csv")
            pq_path = os.path.join(output_dir, f"{self.run_name}_paired.parquet")

            self.df_paired.to_csv(csv_path, index=False)
            self.df_paired.to_parquet(pq_path)

            if verbose:
                print(f"å·²ä¿å­˜: {csv_path}")
                print(f"å·²ä¿å­˜: {pq_path}")
        else:
            if verbose:
                print("æ²¡æœ‰é…å¯¹äº‹ä»¶ï¼Œè·³è¿‡ä¿å­˜")

        return self

    def summary(self) -> Dict[str, Any]:
        """
        è·å–æ•°æ®å¤„ç†æ‘˜è¦ä¿¡æ¯ã€‚

        è¿”å›: åŒ…å«å„ä¸ªå¤„ç†é˜¶æ®µä¿¡æ¯çš„å­—å…¸
        """
        base = {
            "dataset": self.run_name,
            "n_channels": self.n_channels,
            "raw_files_count": sum(len(f) for f in self.raw_files) if self.raw_files else 0,
            "waveforms_shape": [w.shape if w.size > 0 else "empty" for w in self.waveforms],
            "raw_events": len(self.df) if self.df is not None else 0,
            "grouped_events": len(self.df_events) if self.df_events is not None else 0,
            "paired_events": len(self.df_paired) if self.df_paired is not None else 0,
            "features_config": {
                "peaks_range": self.peaks_range,
                "charge_range": self.charge_range,
                "time_window_ns": self.time_window_ns,
            },
        }

        # DAQ ç›¸å…³ä¿¡æ¯ï¼ˆè‹¥å¯ç”¨ä¸”å¯ç”¨ï¼‰
        daq_summary = {"enabled": bool(self.use_daq_scan), "found": False}

        if self.daq_run is not None:
            try:
                self.daq_run.compute_acquisition_times()
            except Exception:
                pass

            # æ›´ç¨³å¥åœ°è®¿é—® DAQRun å±æ€§ï¼Œé˜²æ­¢ç¼ºå¤±å¯¼è‡´å¼‚å¸¸
            daq_summary.update({
                "found": True,
                "run_path": getattr(self.daq_run, "run_path", None),
                "channels": sorted(list(self.daq_run.channels))
                if getattr(self.daq_run, "channels", None) is not None
                else [],
                "channel_count": len(self.daq_run.channels)
                if getattr(self.daq_run, "channels", None) is not None
                else 0,
            })

            chs = {}
            for ch, s in self.daq_run.get_channel_summary().items():
                chs[str(ch)] = {
                    "file_count": s.get("file_count"),
                    "total_size_bytes": s.get("total_size_bytes"),
                    "start_time_ps": s.get("start_time_ps"),
                    "end_time_ps": s.get("end_time_ps"),
                    "duration_s": s.get("duration_s"),
                }
            daq_summary["channels_summary"] = chs

        elif self.daq_info is not None:
            daq_summary.update({
                "found": True,
                "run_path": self.daq_info.get("path"),
                "channels": self.daq_info.get("channels", []),
            })
            chs = {}
            for ch_str, chdata in (self.daq_info.get("channel_details") or {}).items():
                chs[str(ch_str)] = {
                    "file_count": chdata.get("file_count"),
                    "files": [f.get("filename") for f in chdata.get("files", [])],
                }
            daq_summary["channels_summary"] = chs

        base["daq"] = daq_summary
        return base

    @classmethod
    def from_daq_report(
        cls,
        run_name: str,
        daq_report: Union[str, dict],
        data_root: str = "DAQ",
        load_waveforms: bool = True,
        run_pipeline: bool = True,
        n_channels: int = 2,
        start_channel_slice: int = 6,
        time_window_ns: Optional[float] = None,
    ) -> "WaveformDataset":
        """åŸºäº DAQ JSON æŠ¥å‘Šæˆ– report dict åˆ›å»ºå¹¶å¯é€‰åœ°è¿è¡Œå®Œæ•´å¤„ç†æµç¨‹çš„ä¾¿åˆ©å·¥å‚ã€‚

        å‚æ•°:
            run_name: è¿è¡Œå
            daq_report: JSON æ–‡ä»¶è·¯å¾„æˆ–å·²åŠ è½½çš„ dict
            data_root: DAQ æ ¹ç›®å½•ï¼ˆä»…ç”¨äºç›¸å¯¹è·¯å¾„è§£æï¼‰
            load_waveforms: æ˜¯å¦åœ¨ç®¡é“ä¸­åŠ è½½åŸå§‹æ³¢å½¢
            run_pipeline: æ˜¯å¦è¿è¡Œæ•´ä¸ªå¤„ç†æµç¨‹ï¼ˆåŠ è½½ -> æå– -> ç»“æ„åŒ– -> ç‰¹å¾ -> df -> èšç±» -> é…å¯¹ï¼‰
            n_channels: å¤„ç†é€šé“æ•°
            start_channel_slice: èµ·å§‹é€šé“ç´¢å¼•
            time_window_ns: å¯é€‰çš„æ—¶é—´çª—å£è¦†ç›–é»˜è®¤å€¼

        è¿”å›: WaveformDataset å®ä¾‹
        """
        ds = cls(
            run_name=run_name,
            n_channels=n_channels,
            start_channel_slice=start_channel_slice,
            data_root=data_root,
            load_waveforms=load_waveforms,
            use_daq_scan=True,
            daq_root=data_root,
            daq_report=daq_report,
        )

        # åˆå§‹åŒ– DAQ ä¿¡æ¯å¹¶åŠ è½½ raw_files
        ds.check_daq_status()
        ds.load_raw_data(verbose=False)

        if run_pipeline:
            # æ‰§è¡Œå…¸å‹ pipeline
            ds.extract_waveforms(verbose=False)
            ds.structure_waveforms(verbose=False)
            ds.build_waveform_features(verbose=False)
            ds.build_dataframe(verbose=False)
            if time_window_ns is not None:
                ds.group_events(time_window_ns=time_window_ns, verbose=False)
            else:
                ds.group_events(verbose=False)
            ds.pair_events(verbose=False)

        return ds

    def help(self, topic: Optional[str] = None, verbose: bool = False) -> str:
        """
        æ˜¾ç¤ºæ•°æ®é›†ä½¿ç”¨å¸®åŠ©

        Args:
            topic: å¸®åŠ©ä¸»é¢˜ï¼ˆNone/'workflow' æ˜¾ç¤ºé“¾å¼è°ƒç”¨æµç¨‹ï¼Œå…¶ä»–ä¸»é¢˜è½¬å‘ç»™ Contextï¼‰
            verbose: æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯

        Examples:
            >>> ds.help()  # æ˜¾ç¤ºå·¥ä½œæµç¨‹
            >>> ds.help('workflow')  # æ˜¾ç¤ºå·¥ä½œæµç¨‹ï¼ˆåŒä¸Šï¼‰
            >>> ds.help('config')  # è½¬å‘ç»™ ctx.help('config')
        """
        if topic is None or topic in ['workflow', 'chain']:
            return self._show_workflow_help(verbose)
        else:
            # è½¬å‘ç»™ Context
            return self.ctx.help(topic, verbose=verbose)

    def _show_workflow_help(self, verbose: bool = False) -> str:
        """æ˜¾ç¤º WaveformDataset å·¥ä½œæµç¨‹å¸®åŠ©"""
        help_text = """
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘ WaveformDataset å·¥ä½œæµç¨‹                                                     â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ğŸ“‹ æ ‡å‡†åˆ†ææµç¨‹ï¼ˆé“¾å¼è°ƒç”¨ï¼‰:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
from waveform_analysis import WaveformDataset

ds = WaveformDataset(run_name='your_run', n_channels=2)
(ds
    .load_raw_data()            # 1. åŠ è½½ CSV åŸå§‹æ•°æ®
    .extract_waveforms()        # 2. æå–æ³¢å½¢æ•°ç»„
    .structure_waveforms()      # 3. è½¬æ¢ä¸ºç»“æ„åŒ–æ•°ç»„
    .build_waveform_features()  # 4. è®¡ç®— peaks/charges
    .build_dataframe()          # 5. æ„å»º DataFrame
    .group_events(time_window_ns=100)  # 6. æŒ‰æ—¶é—´çª—å£åˆ†ç»„
    .pair_events())             # 7. ç­›é€‰æˆå¯¹äº‹ä»¶

# è·å–ç»“æœ
df_paired = ds.get_paired_events()
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

ğŸ’¡ å†…å­˜ä¼˜åŒ–æŠ€å·§:
  ds = WaveformDataset(run_name='...', load_waveforms=False)
  # è·³è¿‡æ­¥éª¤ 2-3ï¼ŒèŠ‚çœ 70-80% å†…å­˜

ğŸ”— æ›´å¤šå¸®åŠ©:
  ds.help('config')       # é…ç½®ç®¡ç†
  ds.help('quickstart')   # å¿«é€Ÿå¼€å§‹æ¨¡æ¿
"""

        if verbose:
            help_text += """
ğŸ“Š æ•°æ®æµè¯¦è§£:
  1. load_raw_data()         â†’ self.raw_data (List[pd.DataFrame])
  2. extract_waveforms()     â†’ self.waveforms (List[np.ndarray])
  3. structure_waveforms()   â†’ self.st_waveforms (np.ndarray, structured)
  4. build_waveform_features() â†’ self.char/self.peaks/self.charges
  5. build_dataframe()       â†’ self.df (pd.DataFrame)
  6. group_events()          â†’ self.df_grouped (pd.DataFrame)
  7. pair_events()           â†’ self.df_paired (pd.DataFrame)

ğŸ“ æ•°æ®è®¿é—®æ–¹æ³•:
  ds.get_raw_events()        # è·å–åŸå§‹äº‹ä»¶
  ds.get_grouped_events()    # è·å–åˆ†ç»„äº‹ä»¶
  ds.get_paired_events()     # è·å–é…å¯¹äº‹ä»¶
  ds.get_waveform_at(index)  # è·å–æŒ‡å®šæ³¢å½¢
  ds.summary()               # æ˜¾ç¤ºæ‘˜è¦ä¿¡æ¯

ğŸ¯ å¿«é€Ÿå¼€å§‹:
  â€¢ åŸºç¡€åˆ†æ: ds.ctx.quickstart('basic')
"""

        print(help_text)
        return help_text

    def __repr__(self) -> str:
        """æ•°æ®é›†å¯¹è±¡çš„å­—ç¬¦ä¸²è¡¨ç¤ºã€‚"""
        return (
            f"WaveformDataset(run_name='{self.run_name}', n_channels={self.n_channels}, "
            f"raw_events={len(self.df) if self.df is not None else 0}, "
            f"paired_events={len(self.df_paired) if self.df_paired is not None else 0})"
        )
