# CLAUDE.md

This file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.

## Project Overview

WaveformAnalysis is a Python package for processing and analyzing DAQ (Data Acquisition) system waveform data. It features a **plugin-based architecture** inspired by strax, with support for both static and streaming data processing, automatic caching with lineage tracking, and memory-optimized workflows.

**Key Characteristics:**
- Plugin-based processing with automatic dependency resolution (DAG)
- Context-managed stateless execution (explicit `run_id` required)
- Zero-copy caching via `numpy.memmap` with atomic writes
- Streaming support for memory-efficient processing of large datasets
- Global executor management for thread/process pool reuse

## Development Commands

### Installation
```bash
# Quick install (recommended)
./install.sh

# Manual install (development mode)
pip install -e .

# With development dependencies
pip install -e ".[dev]"
```

### Testing
```bash
# Run tests (auto-activates conda env pyroot-kernel)
./scripts/run_tests.sh

# Or via Makefile
make test

# Run tests with specific pytest args
./scripts/run_tests.sh -v -k test_name

# Custom conda environment
CONDA_ENV=my-env ./scripts/run_tests.sh
```

### Benchmarking
```bash
# Run I/O benchmark
make bench

# Custom benchmark parameters
python scripts/benchmark_io.py --n-files 100 --n-channels 2 --n-samples 500 --reps 3
```

### Code Quality
```bash
# Format code (Black)
black waveform_analysis/ --line-length 100

# Type checking
mypy waveform_analysis/

# Run tests with coverage
pytest -v --cov=waveform_analysis --cov-report=html
```

### CLI Usage
```bash
# Process a run
waveform-process --run-name 50V_OV_circulation_20thr --verbose

# Scan DAQ directory
waveform-process --scan-daq --daq-root DAQ

# Show DAQ overview
waveform-process --show-daq --daq-root DAQ
```

## Configuration Management

WaveformAnalysis æä¾›çµæ´»çš„é…ç½®ç³»ç»Ÿï¼Œæ”¯æŒå…¨å±€é…ç½®å’Œæ’ä»¶ç‰¹å®šé…ç½®ã€‚

### æŸ¥çœ‹æ’ä»¶é…ç½®é€‰é¡¹

```python
# åˆ—å‡ºæ‰€æœ‰æ’ä»¶çš„é…ç½®é€‰é¡¹
ctx.list_plugin_configs()

# åªæŸ¥çœ‹ç‰¹å®šæ’ä»¶çš„é…ç½®
ctx.list_plugin_configs(plugin_name='waveforms')

# è·å–é…ç½®å­—å…¸è€Œä¸æ‰“å°ï¼ˆç”¨äºç¨‹åºåŒ–å¤„ç†ï¼‰
config_info = ctx.list_plugin_configs(verbose=False)
```

`list_plugin_configs()` åŠŸèƒ½ç‰¹æ€§ï¼š
- ğŸ“¦ æ˜¾ç¤ºæ‰€æœ‰æ’ä»¶çš„é…ç½®é€‰é¡¹ã€é»˜è®¤å€¼ã€ç±»å‹å’Œå¸®åŠ©æ–‡æœ¬
- âœ“/âš™ï¸ å›¾æ ‡åŒºåˆ†é»˜è®¤å€¼å’Œå·²ä¿®æ”¹çš„é…ç½®
- ğŸ”§ æ˜ç¡®æ ‡è®°å·²è‡ªå®šä¹‰çš„é…ç½®å€¼
- ğŸ“Š ç»Ÿè®¡å·²æ³¨å†Œæ’ä»¶æ•°ã€é…ç½®é€‰é¡¹æ€»æ•°å’Œå·²ä¿®æ”¹é…ç½®æ•°
- ğŸ“ è‡ªåŠ¨æ¢è¡Œå¤„ç†é•¿æè¿°å’Œå¸®åŠ©æ–‡æœ¬
- ğŸ¨ æ¸…æ™°çš„è¡¨æ ¼è¾¹æ¡†å’Œå±‚æ¬¡ç»“æ„

### æŸ¥çœ‹å½“å‰é…ç½®å€¼

```python
# æ˜¾ç¤ºå…¨å±€é…ç½®ï¼ˆåŒ…å«é…ç½®é¡¹ä½¿ç”¨æƒ…å†µï¼‰
ctx.show_config()

# æ˜¾ç¤ºç‰¹å®šæ’ä»¶çš„è¯¦ç»†é…ç½®
ctx.show_config('waveforms')

# æ˜¾ç¤ºå…¨å±€é…ç½®ä½†ä¸æ˜¾ç¤ºä½¿ç”¨æƒ…å†µ
ctx.show_config(show_usage=False)
```

`show_config()` å¢å¼ºåŠŸèƒ½ç‰¹æ€§ï¼š
- ğŸ” **æ™ºèƒ½åˆ†æé…ç½®é¡¹ä½¿ç”¨æƒ…å†µ** - è‡ªåŠ¨è¯†åˆ«å“ªäº›æ’ä»¶ä½¿ç”¨äº†æ¯ä¸ªå…¨å±€é…ç½®é¡¹
- ğŸ“‚ **ä¸‰ç±»é…ç½®åˆ†ç»„æ˜¾ç¤º**ï¼š
  - **å…¨å±€é…ç½®é¡¹** - è¢«æ’ä»¶ä½¿ç”¨çš„é…ç½®ï¼Œæ˜¾ç¤ºä½¿ç”¨æ’ä»¶åˆ—è¡¨
  - **æ’ä»¶ç‰¹å®šé…ç½®** - ä»…å¯¹å•ä¸ªæ’ä»¶ç”Ÿæ•ˆçš„é…ç½®ï¼ˆåµŒå¥—å­—å…¸æˆ–ç‚¹åˆ†éš”ï¼‰
  - **æœªä½¿ç”¨é…ç½®** - æœªè¢«ä»»ä½•æ’ä»¶ä½¿ç”¨çš„é…ç½®é¡¹ï¼ˆå¸®åŠ©å‘ç°é…ç½®é”™è¯¯ï¼‰
- âš™ï¸ **è¯¦ç»†çš„æ’ä»¶é…ç½®è§†å›¾** - æŸ¥çœ‹ç‰¹å®šæ’ä»¶æ—¶æ˜¾ç¤ºå®Œæ•´ä¿¡æ¯ï¼š
  - é…ç½®å€¼ä¸é»˜è®¤å€¼å¯¹æ¯”
  - é…ç½®é¡¹ç±»å‹å’Œè¯´æ˜
  - è‡ªå®šä¹‰çŠ¶æ€æ ‡è®°
- ğŸ“Š **ç»Ÿè®¡æ¦‚è§ˆ** - ä¸€ç›®äº†ç„¶åœ°çœ‹åˆ°é…ç½®é¡¹åˆ†å¸ƒæƒ…å†µ

### è®¾ç½®é…ç½®

```python
# å…¨å±€é…ç½®
ctx.set_config({'n_channels': 2, 'threshold': 50})

# æ’ä»¶ç‰¹å®šé…ç½®ï¼ˆæ¨èï¼Œé¿å…å†²çªï¼‰
ctx.set_config({'threshold': 50}, plugin_name='peaks')

# æŸ¥çœ‹å½“å‰é…ç½®å€¼
ctx.show_config('plugin_name')
```

## Architecture Overview

### Core Structure (Modular Subdirectories)

ä» 2026-01 ç‰ˆæœ¬å¼€å§‹ï¼Œ`core/` ç›®å½•é‡‡ç”¨**æ¨¡å—åŒ–å­ç›®å½•æ¶æ„**ï¼Œå°†åŸæœ¬æ‰å¹³çš„ 27 ä¸ªæ–‡ä»¶é‡æ„ä¸º 6 ä¸ªåŠŸèƒ½å­ç›®å½•ï¼š

- **`storage/`**: å­˜å‚¨å±‚ï¼ˆmemmap, backends, cache, compression, integrityï¼‰
- **`execution/`**: æ‰§è¡Œå±‚ï¼ˆmanager, config, timeoutï¼‰
- **`plugins/`**: æ’ä»¶ç³»ç»Ÿï¼ˆåˆ†ç¦» core/ å’Œ builtin/ï¼‰
- **`processing/`**: æ•°æ®å¤„ç†ï¼ˆloader, processor, analyzer, chunkï¼‰
- **`data/`**: æ•°æ®ç®¡ç†ï¼ˆquery, exportï¼‰
- **`foundation/`**: æ¡†æ¶åŸºç¡€ï¼ˆexceptions, mixins, model, utils, progressï¼‰

æ ¸å¿ƒæ–‡ä»¶ `context.py` å’Œ `dataset.py` ä¿æŒåœ¨ core/ æ ¹ç›®å½•ã€‚

### Core Components

1. **Context Layer** (`core/context.py`)
   - Central coordinator managing plugins, configuration, and caching
   - **Stateless**: All operations require explicit `run_id` parameter
   - Data stored in `_results[(run_id, data_name)]`
   - Automatic dependency resolution with cycle detection
   - Lineage-based caching with SHA1 hashing of plugin code, version, config, dtype

2. **Plugin System** (`core/plugins/`)
   - **Modular**: Core infrastructure (`plugins/core/`) ä¸å†…ç½®æ’ä»¶ï¼ˆ`plugins/builtin/`ï¼‰åˆ†ç¦»
   - **Accelerator-based Architecture** (since 2026-01): æŒ‰åŠ é€Ÿå™¨åˆ’åˆ†æ’ä»¶
     - `builtin/cpu/`: CPU å®ç°ï¼ˆNumPy/SciPy/Numbaï¼‰
     - `builtin/jax/`: JAX GPU å®ç°ï¼ˆå¾…å¼€å‘ï¼‰
     - `builtin/streaming/`: æµå¼å¤„ç†æ’ä»¶ï¼ˆå¾…å¼€å‘ï¼‰
     - `builtin/legacy/`: å‘åå…¼å®¹å±‚ï¼ˆå¼ƒç”¨è­¦å‘Šï¼‰
   - Each plugin declares: `provides`, `depends_on`, `options`, `version`, `dtype`
   - `compute()` returns ndarray or generator
   - `is_side_effect=True` isolates outputs to `_side_effects/{run_id}/{plugin_name}`
   - **CPU Standard Plugins**:
     - Data processing: RawFiles â†’ Waveforms â†’ StWaveforms â†’ Features â†’ DataFrame â†’ GroupedEvents â†’ PairedEvents
     - Signal processing: FilteredWaveforms (Butterworth/Savitzky-Golay), SignalPeaks (scipy.signal)
   - **Plugin Organization**:
     - `cpu/standard.py`: 10ä¸ªæ ‡å‡†æ•°æ®å¤„ç†æ’ä»¶
     - `cpu/filtering.py`: FilteredWaveformsPlugin
     - `cpu/peak_finding.py`: SignalPeaksPlugin

3. **Storage Layer** (`core/storage/`)
   - `MemmapStorage` (`storage/memmap.py`): Zero-copy array persistence with atomic writes (`.tmp` â†’ rename)
   - `StorageBackend` (`storage/backends.py`): Pluggable backends (SQLite, etc.)
   - `CacheManager` (`storage/cache.py`): Lineage-based cache validation
   - `CompressionManager` (`storage/compression.py`): Blosc2, LZ4, Zstd, Gzip
   - `IntegrityChecker` (`storage/integrity.py`): Checksum validation
   - Validates `dtype.descr` and `STORAGE_VERSION`
   - File locking for concurrent access protection
   - Watch signature (`WATCH_SIG_KEY`) tracks input file mtime/size for cache invalidation

4. **Streaming Framework** (`core/plugins/core/streaming.py`, `core/plugins/builtin/streaming_examples.py`)
   - `StreamingPlugin`: Returns chunk iterators instead of static data
   - `StreamingContext`: Manages chunk flows with automatic parallelization
   - Time-aligned chunk processing with boundary validation
   - Mixed static/streaming plugin support

5. **Executor Management** (`core/execution/`)
   - `ExecutorManager` (`execution/manager.py`): Global singleton for thread/process pool reuse
   - `EXECUTOR_CONFIGS` (`execution/config.py`): Predefined configs: `io_intensive`, `cpu_intensive`, `large_data`, `small_data`
   - `TimeoutManager` (`execution/timeout.py`): Timeout control
   - Context manager support: `with get_executor('io_intensive') as executor:`
   - Helper functions: `parallel_map()`, `parallel_apply()`

6. **Dataset API** (`core/dataset.py`)
   - High-level chainable interface wrapping Context
   - Memory optimization: `load_waveforms=False` skips waveform extraction (saves 70-80% memory)
   - Feature registration system
   - Timestamp indexing for fast `get_waveform_at()` lookups

7. **Chunk Utilities** (`core/processing/chunk.py`)
   - `Chunk(data, start, end, run_id, ...)`: Encapsulates data with time boundaries
   - Time range operations: `select_time_range()`, `clip_to_time_range()`
   - Validation: `check_monotonic()`, `check_no_overlap()`, `check_chunk_boundaries()`
   - Splitting/merging: `split_by_time()`, `merge_chunks()`, `rechunk()`

8. **Time Range Query** (`core/data/query.py`) [NEW - Phase 2.2]
   - `TimeIndex`: Efficient time indexing with O(log n) binary search queries
   - `TimeRangeQueryEngine`: Manages multiple data type indices
   - Context integration: `get_data_time_range()`, `build_time_index()`, `clear_time_index()`
   - Query result caching for repeated queries
   - Example: `ctx.get_data_time_range('run_001', 'st_waveforms', start_time=1000, end_time=2000)`

9. **Strax Plugin Adapter** (`core/plugins/core/adapters.py`) [NEW - Phase 2.3]
   - `StraxPluginAdapter`: Wraps strax plugins for seamless integration
   - `StraxContextAdapter`: Provides strax-style API (`get_array`, `get_df`, `search_field`)
   - Automatic metadata extraction and parameter mapping
   - Configuration option compatibility
   - Example: `adapter = wrap_strax_plugin(MyStraxPlugin); ctx.register_plugin(adapter)`

10. **Batch Processing & Export** (`core/data/export.py`) [NEW - Phase 3.1 & 3.2]
    - `BatchProcessor`: Parallel/serial processing of multiple runs
    - `DataExporter`: Unified export interface (Parquet, HDF5, CSV, JSON, NumPy)
    - Progress tracking and flexible error handling
    - Example: `processor.process_runs(run_ids, 'peaks', max_workers=4)`
    - Example: `exporter.export(data, 'output.parquet')`

11. **Hot Reload** (`core/plugins/core/hot_reload.py`) [NEW - Phase 3.3]
    - `PluginHotReloader`: File change monitoring and automatic module reloading
    - Cache consistency maintenance after reload
    - Auto-reload daemon thread support
    - Example: `reloader = enable_hot_reload(ctx, ['my_plugin'], auto_reload=True)`

### Data Flow (Standard Pipeline)

```
CSV Files â†’ RawFilesPlugin â†’ WaveformsPlugin â†’ StWaveformsPlugin
                                                       â†“
                                              â”Œâ”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”
                                              â†“               â†“
                                        PeaksPlugin    ChargesPlugin
                                              â†“               â†“
                                              â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                                                      â†“
                                               DataFramePlugin
                                                      â†“
                                            GroupedEventsPlugin
                                               (Numba + MP)
                                                      â†“
                                            PairedEventsPlugin
```

## Important Conventions

### Architecture & Responsibility Separation
- **Context**: Only manages plugin DAG, config, lineage, and caching
- **Dataset**: Provides chainable interface and result access (delegates to Context)
- **Never** maintain parallel state in Dataset; map attributes (like `self.char` â†’ Context) to ensure single source of truth
- **Plugin Responsibility**: Each plugin does ONE thing; add new features as new plugins, not by expanding existing ones

### Run Identification
- **Always use `run_name`** instead of `char` (legacy term being phased out)
- Pass `run_id` explicitly to all `Context.get_data()` calls
- Missing `run_id` causes data overwrites and lineage conflicts

### Module Exports (`core/foundation/utils.py`)
All new modules **must** use the `exporter()` pattern:

```python
from waveform_analysis.core.foundation.utils import exporter
export, __all__ = exporter()

@export
class MyClass: ...

@export(name="AlternativeName")
def my_func(): ...

MY_CONST = export(42, name="MY_CONST")
```

### Naming Conventions
- Classes: `PascalCase`
- Functions/variables: `snake_case`
- Constants: `UPPER_SNAKE_CASE`
- Terminology: Use consistent business terms (waveforms/events/hits/chunks) across code and docs
- Event size: Use `event_length`, not `pair_len` or `pair_length`

### Cache Management
- Step-level cache: `set_step_cache(step, enabled=True, attrs=[...], persist_path=..., watch_attrs=[...])`
- Persistent cache writes `WATCH_SIG_KEY="__watch_sig__"` (mtime/size SHA1) for validation
- Cache automatically invalidates on plugin version/config/dtype changes
- Generator outputs consumed once; re-access triggers recomputation

### Time & Chunk Operations
- Records have `time`, `dt`, `length` fields; `endtime = time + dt * length`
- Chunk boundaries must be respected: record endtime â‰¤ chunk end
- Use `Chunk` objects for time-aligned data processing
- Validate with `check_sorted_by_time()` and `check_chunk_boundaries()`

### Performance Optimization
- **Numba JIT**: Available for hot loops (e.g., `group_multi_channel_hits` with `use_numba=True`)
- **Multiprocessing**: Use for large-scale CPU-bound tasks
- **Vectorization**: Prefer NumPy broadcasting over explicit loops
- **IO parallelization**: Use `ExecutorManager` with `io_intensive` config
- **CPU parallelization**: Use `ExecutorManager` with `cpu_intensive` config

## Plugin Architecture and Import Guide

### Accelerator-Based Plugin Organization (Since 2026-01)

æ’ä»¶æŒ‰ç…§è®¡ç®—åŠ é€Ÿå™¨ç±»å‹ç»„ç»‡ï¼Œæ”¯æŒ CPUã€JAXï¼ˆGPUï¼‰å’Œæµå¼å¤„ç†ï¼š

```
builtin/
â”œâ”€â”€ cpu/              # CPU å®ç° (NumPy/SciPy/Numba)
â”‚   â”œâ”€â”€ standard.py   # æ ‡å‡†æ•°æ®å¤„ç†æ’ä»¶
â”‚   â”œâ”€â”€ filtering.py  # æ»¤æ³¢æ’ä»¶
â”‚   â””â”€â”€ peak_finding.py # å¯»å³°æ’ä»¶
â”œâ”€â”€ jax/              # JAX GPU å®ç°ï¼ˆå¾…å¼€å‘ï¼‰
â”œâ”€â”€ streaming/        # æµå¼å¤„ç†æ’ä»¶ï¼ˆå¾…å¼€å‘ï¼‰
â””â”€â”€ legacy/           # å‘åå…¼å®¹ï¼ˆå¼ƒç”¨ï¼‰
```

### Plugin Import Methods

```python
# æ–¹æ³• 1: ä» cpu/ ç›´æ¥å¯¼å…¥ï¼ˆæ¨èï¼Œæ˜ç¡®æŒ‡å®šåŠ é€Ÿå™¨ï¼‰
from waveform_analysis.core.plugins.builtin.cpu import (
    RawFilesPlugin,
    WaveformsPlugin,
    FilteredWaveformsPlugin,
    SignalPeaksPlugin,
)

# æ–¹æ³• 2: ä» builtin/ å¯¼å…¥ï¼ˆå‘åå…¼å®¹ï¼Œé»˜è®¤ä½¿ç”¨ CPU å®ç°ï¼‰
from waveform_analysis.core.plugins.builtin import (
    RawFilesPlugin,
    WaveformsPlugin,
    FilteredWaveformsPlugin,
    SignalPeaksPlugin,
)

# æ–¹æ³• 3: ä» legacy/ å¯¼å…¥ï¼ˆä¸æ¨èï¼Œä¼šå‘å‡ºå¼ƒç”¨è­¦å‘Šï¼‰
from waveform_analysis.core.plugins.builtin.legacy import RawFilesPlugin
# DeprecationWarning: RawFilesPlugin å·²è¢«å¼ƒç”¨ï¼Œå°†åœ¨ä¸‹ä¸€ä¸ªä¸»ç‰ˆæœ¬ä¸­ç§»é™¤...
```

### Available CPU Plugins

#### æ ‡å‡†æ•°æ®å¤„ç†æ’ä»¶ (`cpu/standard.py`)
- `RawFilesPlugin`: æ‰«æå’Œåˆ†ç»„åŸå§‹ CSV æ–‡ä»¶
- `WaveformsPlugin`: æå–æ³¢å½¢æ•°æ®
- `StWaveformsPlugin`: ç»“æ„åŒ–æ³¢å½¢æ•°ç»„
- `HitFinderPlugin`: æ£€æµ‹ Hit äº‹ä»¶
- `PeaksPlugin`: å³°å€¼ç‰¹å¾è®¡ç®—ï¼ˆç›´æ¥ä¾èµ– st_waveformsï¼‰
- `ChargesPlugin`: ç”µè·ç§¯åˆ†è®¡ç®—ï¼ˆç›´æ¥ä¾èµ– st_waveformsï¼‰
- `DataFramePlugin`: æ„å»º DataFrame
- `GroupedEventsPlugin`: æ—¶é—´çª—å£åˆ†ç»„ï¼ˆæ”¯æŒ Numba åŠ é€Ÿï¼‰
- `PairedEventsPlugin`: è·¨é€šé“äº‹ä»¶é…å¯¹

#### ä¿¡å·å¤„ç†æ’ä»¶
- `FilteredWaveformsPlugin` (`cpu/filtering.py`): æ³¢å½¢æ»¤æ³¢
  - Butterworth å¸¦é€šæ»¤æ³¢å™¨
  - Savitzky-Golay æ»¤æ³¢å™¨
- `SignalPeaksPlugin` (`cpu/peak_finding.py`): é«˜çº§å³°å€¼æ£€æµ‹
  - åŸºäº scipy.signal.find_peaks
  - æ”¯æŒå¯¼æ•°æ£€æµ‹ã€é«˜åº¦ã€è·ç¦»ã€æ˜¾è‘—æ€§ç­‰å‚æ•°
  - è¿”å› `ADVANCED_PEAK_DTYPE` ç»“æ„åŒ–æ•°ç»„

### Migration Guide from Legacy Plugins

å¦‚æœä½ çš„ä»£ç ä½¿ç”¨äº†æ—§çš„å¯¼å…¥æ–¹å¼ï¼Œå»ºè®®è¿ç§»åˆ°æ–°æ¶æ„ï¼š

```python
# æ—§æ–¹å¼ï¼ˆä¼šå‘å‡ºè­¦å‘Šï¼‰
from waveform_analysis.core.plugins.builtin.standard import RawFilesPlugin
from waveform_analysis.core.plugins.builtin.signal_processing import FilteredWaveformsPlugin

# æ–°æ–¹å¼ï¼ˆæ¨èï¼‰
from waveform_analysis.core.plugins.builtin.cpu import RawFilesPlugin, FilteredWaveformsPlugin
```

## Common Patterns

### Basic Dataset Usage
```python
from waveform_analysis import WaveformDataset

ds = WaveformDataset(run_name="50V_OV_circulation_20thr", n_channels=2)
(ds
    .load_raw_data()
    .extract_waveforms()
    .structure_waveforms()
    .build_waveform_features()
    .build_dataframe()
    .group_events(time_window_ns=100)
    .pair_events())

df = ds.get_paired_events()
```

### Memory-Optimized Workflow
```python
# Skip waveform extraction (saves 70-80% memory)
ds = WaveformDataset(run_name="...", load_waveforms=False)
ds.load_raw_data().extract_waveforms().structure_waveforms()  # All skipped
ds.build_waveform_features()  # Still computes features
ds.get_waveform_at(0)  # Returns None with warning
```

### Plugin-Based Context Usage
```python
from waveform_analysis.core.context import Context
from waveform_analysis.core.standard_plugins import *

ctx = Context(storage_dir="./strax_data")
ctx.register_plugin(RawFilesPlugin())
ctx.register_plugin(WaveformsPlugin())
ctx.set_config({"data_root": "DAQ", "n_channels": 2})

hits = ctx.get_data("run_001", "hits")  # Auto-resolves dependencies
```

### Preview Execution (è¿è¡Œå‰ç¡®è®¤ Lineage)
```python
# åœ¨å®é™…æ‰§è¡Œå‰é¢„è§ˆæ‰§è¡Œè®¡åˆ’
ctx.preview_execution('run_001', 'signal_peaks')

# è¾“å‡ºåŒ…å«ï¼š
# - æ‰§è¡Œè®¡åˆ’ï¼ˆæ’ä»¶æ‰§è¡Œé¡ºåºï¼‰
# - ä¾èµ–å…³ç³»æ ‘
# - è‡ªå®šä¹‰é…ç½®å‚æ•°ï¼ˆä»…æ˜¾ç¤ºéé»˜è®¤å€¼ï¼‰
# - ç¼“å­˜çŠ¶æ€ï¼ˆå“ªäº›å·²ç¼“å­˜ï¼Œå“ªäº›éœ€è¦è®¡ç®—ï¼‰

# ç¨‹åºåŒ–ä½¿ç”¨é¢„è§ˆç»“æœ
result = ctx.preview_execution('run_001', 'signal_peaks')
needs_compute = [p for p, s in result['cache_status'].items() if s['needs_compute']]
print(f"éœ€è¦è®¡ç®— {len(needs_compute)} ä¸ªæ’ä»¶")

# ç¡®è®¤åæ‰§è¡Œ
data = ctx.get_data('run_001', 'signal_peaks')

# ä¸åŒè¯¦ç»†ç¨‹åº¦
ctx.preview_execution('run_001', 'signal_peaks', verbose=0)  # ç®€æ´
ctx.preview_execution('run_001', 'signal_peaks', verbose=1)  # æ ‡å‡†ï¼ˆé»˜è®¤ï¼‰
ctx.preview_execution('run_001', 'signal_peaks', verbose=2)  # è¯¦ç»†

# é€‰æ‹©æ€§æ˜¾ç¤º
ctx.preview_execution('run_001', 'signal_peaks',
                      show_tree=False,   # ä¸æ˜¾ç¤ºä¾èµ–æ ‘
                      show_config=True,  # æ˜¾ç¤ºé…ç½®
                      show_cache=True)   # æ˜¾ç¤ºç¼“å­˜çŠ¶æ€
```

### Streaming Processing
```python
from waveform_analysis.core.streaming import get_streaming_context

stream_ctx = get_streaming_context(ctx, run_id="run_001", chunk_size=50000)
for chunk in stream_ctx.get_stream("st_waveforms_stream"):
    process_chunk(chunk)
```

### Adding Custom Features
```python
def my_feature_fn(self, st_waveforms, event_length, **params):
    # Returns list of feature arrays (one per channel)
    return [np.array(...) for _ in range(self.n_channels)]

ds.register_feature("my_feature", my_feature_fn, param1=value1)
ds.compute_registered_features()  # Called during build_dataframe()
```

### Using New Features (Phase 2 & 3)

#### Time Range Queries
```python
from waveform_analysis.core.context import Context

ctx = Context()
# ... register plugins and set config ...

# Query specific time range
data = ctx.get_data_time_range(
    'run_001', 'st_waveforms',
    start_time=1000000,
    end_time=2000000
)

# Pre-build index for better performance
ctx.build_time_index('run_001', 'st_waveforms', endtime_field='computed')

# Get index statistics
stats = ctx.get_time_index_stats()
print(f"Total indices: {stats['total_indices']}")
```

#### Strax Plugin Integration
```python
from waveform_analysis.core.strax_adapter import (
    wrap_strax_plugin,
    create_strax_context
)

# Wrap existing strax plugin
adapter = wrap_strax_plugin(MyStraxPlugin)
ctx.register_plugin(adapter)

# Or use strax-style API
strax_ctx = create_strax_context('./data')
strax_ctx.register(MyStraxPlugin)
data = strax_ctx.get_array('run_001', 'peaks')
df = strax_ctx.get_df('run_001', ['peaks', 'hits'])
```

#### Batch Processing
```python
from waveform_analysis.core.batch_export import BatchProcessor

processor = BatchProcessor(ctx)

# Process multiple runs in parallel
results = processor.process_runs(
    run_ids=['run_001', 'run_002', 'run_003'],
    data_name='peaks',
    max_workers=4,
    show_progress=True,
    on_error='continue'  # 'continue', 'stop', or 'raise'
)

# Access results
for run_id, data in results['results'].items():
    print(f"{run_id}: {len(data)} events")

# Check errors
if results['errors']:
    print(f"Errors: {results['errors']}")
```

#### Data Export
```python
from waveform_analysis.core.batch_export import DataExporter, batch_export

# Export single dataset
exporter = DataExporter()
exporter.export(data, 'output.parquet')  # Auto-detect format
exporter.export(data, 'output.hdf5', key='waveforms')
exporter.export(data, 'output.csv')

# Batch export multiple runs
batch_export(
    ctx,
    run_ids=['run_001', 'run_002'],
    data_name='peaks',
    output_dir='./exports',
    format='parquet',
    max_workers=4
)
```

#### Hot Reload (Development)
```python
from waveform_analysis.core.hot_reload import enable_hot_reload

# Enable auto-reload for development
reloader = enable_hot_reload(
    ctx,
    plugin_names=['my_plugin'],
    auto_reload=True,
    interval=2.0  # Check every 2 seconds
)

# Manually reload after changes
reloader.reload_plugin('my_plugin', clear_cache=True)

# Disable when done
reloader.disable_auto_reload()
```

#### Lineage Visualization (è¡€ç¼˜å›¾å¯è§†åŒ–)

WaveformAnalysis æä¾›ä¸¤ç§é«˜çº§è¡€ç¼˜å›¾å¯è§†åŒ–æ¨¡å¼ï¼Œæ”¯æŒæ™ºèƒ½é¢œè‰²é«˜äº®å’Œå®Œæ•´äº¤äº’åŠŸèƒ½ã€‚

##### LabVIEW é£æ ¼ï¼ˆMatplotlibï¼‰
```python
# åŸºç¡€ç”¨æ³•
ctx.plot_lineage("df_paired", kind="labview")

# äº¤äº’å¼æ¨¡å¼ï¼ˆé¼ æ ‡æ‚¬åœæ˜¾ç¤ºè¯¦æƒ…ã€ç‚¹å‡»æ˜¾ç¤ºä¾èµ–ï¼‰
ctx.plot_lineage("df_paired", kind="labview", interactive=True)

# æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯
ctx.plot_lineage("df_paired", kind="labview", verbose=2, interactive=True)
```

##### Plotly é«˜çº§äº¤äº’å¼
```python
# Plotly æ¨¡å¼ï¼ˆå§‹ç»ˆäº¤äº’å¼ï¼Œæ”¯æŒç¼©æ”¾ã€å¹³ç§»ã€æ‚¬åœï¼‰
ctx.plot_lineage("df_paired", kind="plotly", verbose=2)

# è‡ªå®šä¹‰æ ·å¼
from waveform_analysis.core.foundation.utils import LineageStyle
style = LineageStyle(
    node_width=4.0,
    node_height=2.0,
    verbose=2
)
ctx.plot_lineage("df_paired", kind="plotly", style=style)
```

##### Verbose ç­‰çº§è¯´æ˜
- `verbose=0`: ä»…æ˜¾ç¤ºæ’ä»¶æ ‡é¢˜
- `verbose=1`: æ˜¾ç¤ºæ ‡é¢˜ + key
- `verbose=2`: æ˜¾ç¤ºæ ‡é¢˜ + key + classï¼ˆæ¨èï¼‰
- `verbose>=3`: åŒ verbose=2

##### æ™ºèƒ½é¢œè‰²é«˜äº®

ç³»ç»Ÿè‡ªåŠ¨æ ¹æ®èŠ‚ç‚¹ç±»å‹åº”ç”¨é¢œè‰²æ–¹æ¡ˆï¼š

| èŠ‚ç‚¹ç±»å‹ | é¢œè‰² | è¯†åˆ«è§„åˆ™ |
|---------|------|---------|
| åŸå§‹æ•°æ® | ğŸ”µ è“è‰²ç³» | RawFiles, Loader, Reader |
| ç»“æ„åŒ–æ•°ç»„ | ğŸŸ¢ ç»¿è‰²ç³» | å¤šå­—æ®µ dtypeï¼ˆå¦‚ `[('time', '<f8'), ...]`ï¼‰|
| DataFrame | ğŸŸ  æ©™è‰²ç³» | DataFrame, df å…³é”®è¯ |
| èšåˆæ•°æ® | ğŸŸ£ ç´«è‰²ç³» | Group, Pair, Aggregate, Merge |
| å‰¯ä½œç”¨ | ğŸŒ¸ ç²‰çº¢è‰²ç³» | Export, Save, Write |
| ä¸­é—´å¤„ç† | âšª ç°è‰²ç³» | å…¶ä»–èŠ‚ç‚¹ |

é¢œè‰²é«˜äº®åœ¨ä¸¤ç§æ¨¡å¼ä¸‹å‡è‡ªåŠ¨ç”Ÿæ•ˆï¼Œæ— éœ€é¢å¤–é…ç½®ã€‚

##### Plotly æ¨¡å¼ç‰¹æ€§

- âœ… **çœŸå®çŸ©å½¢ç»˜åˆ¶**ï¼šä½¿ç”¨ shapes API ç»˜åˆ¶èŠ‚ç‚¹å’Œç«¯å£ï¼Œå°ºå¯¸ç²¾ç¡®
- âœ… **å®Œæ•´äº¤äº’æ€§**ï¼šç¼©æ”¾ã€å¹³ç§»ã€æ‚¬åœæç¤ºã€æ¡†é€‰
- âœ… **åæ ‡åŒæ­¥ä¿®å¤**ï¼šæ‹–æ‹½æ—¶å…‰æ ‡å’ŒèŠ‚ç‚¹ä½ç½®å®Œå…¨åŒæ­¥
- âœ… **1:1 æ¯”ä¾‹ä¿æŒ**ï¼šç¡®ä¿èŠ‚ç‚¹ä¸å˜å½¢
- âœ… **ç«¯å£å¯è§**ï¼šæ˜¾ç¤ºå½©è‰²è¾“å…¥/è¾“å‡ºç«¯å£
- âœ… **ç±»å‹æ ‡æ³¨**ï¼šæ‚¬åœæç¤ºåŒ…å«èŠ‚ç‚¹ç±»å‹ä¿¡æ¯

##### æ³¨æ„äº‹é¡¹

1. **Interactive å‚æ•°**ï¼š
   - LabVIEW æ¨¡å¼ï¼š`interactive=True` å¯ç”¨ matplotlib äº¤äº’åŠŸèƒ½
   - Plotly æ¨¡å¼ï¼šå§‹ç»ˆäº¤äº’å¼ï¼Œ`interactive` å‚æ•°è¢«å¿½ç•¥

2. **æ€§èƒ½è€ƒè™‘**ï¼š
   - LabVIEW æ¨¡å¼é€‚åˆé™æ€å¯¼å‡ºå’Œç®€å•äº¤äº’
   - Plotly æ¨¡å¼é€‚åˆå¤æ‚å›¾å½¢çš„æ·±åº¦æ¢ç´¢

3. **ä¾èµ–**ï¼š
   - LabVIEW æ¨¡å¼ï¼šéœ€è¦ matplotlibï¼ˆæ ‡å‡†ä¾èµ–ï¼‰
   - Plotly æ¨¡å¼ï¼šéœ€è¦ `pip install plotly`

## Cache Management (ç¼“å­˜ç®¡ç†)

WaveformAnalysis æä¾›å®Œæ•´çš„ç¼“å­˜ç®¡ç†å·¥å…·é›†ï¼Œç”¨äºåˆ†æã€è¯Šæ–­ã€æ¸…ç†å’Œç»Ÿè®¡ç¼“å­˜æ•°æ®ã€‚

### Python API

#### å¿«é€Ÿä½¿ç”¨

```python
# è·å–ç¼“å­˜åˆ†æå™¨
analyzer = ctx.analyze_cache()

# æŸ¥çœ‹ç¼“å­˜ç»Ÿè®¡
stats = ctx.cache_stats(detailed=True)

# è¯Šæ–­ç¼“å­˜é—®é¢˜
issues = ctx.diagnose_cache(run_id='run_001')

# è‡ªåŠ¨ä¿®å¤ï¼ˆdry-runï¼‰
ctx.diagnose_cache(run_id='run_001', auto_fix=True, dry_run=True)
```

#### é«˜çº§ç”¨æ³•

```python
from waveform_analysis.core.storage import (
    CacheAnalyzer,
    CacheDiagnostics,
    CacheCleaner,
    CacheStatsCollector,
    CleanupStrategy,
)

# 1. åˆ†æç¼“å­˜
analyzer = CacheAnalyzer(ctx)
analyzer.scan()

# è·å–æ‰€æœ‰æ¡ç›®
entries = analyzer.get_entries()

# æŒ‰æ¡ä»¶è¿‡æ»¤
large = analyzer.get_entries(min_size=1024*1024)  # > 1MB
old = analyzer.get_entries(max_age_days=30)       # > 30 å¤©
run_entries = analyzer.get_entries(run_id='run_001')

# æ‰“å°æ‘˜è¦
analyzer.print_summary(detailed=True)

# 2. è¯Šæ–­é—®é¢˜
diag = CacheDiagnostics(analyzer)
issues = diag.diagnose()
diag.print_report(issues)

# è‡ªåŠ¨ä¿®å¤
result = diag.auto_fix(issues, dry_run=True)

# 3. æ™ºèƒ½æ¸…ç†
cleaner = CacheCleaner(analyzer)

# åˆ›å»ºæ¸…ç†è®¡åˆ’
plan = cleaner.plan_cleanup(
    strategy=CleanupStrategy.LRU,
    target_size_mb=1024
)
cleaner.preview_plan(plan, detailed=True)

# æ‰§è¡Œæ¸…ç†
cleaner.execute(plan, dry_run=False)

# æŒ‰å¹´é¾„æ¸…ç†
cleaner.cleanup_by_age(max_age_days=30, dry_run=True)

# æ¸…ç†åˆ°ç›®æ ‡å¤§å°
cleaner.cleanup_to_target_size(target_total_mb=500, dry_run=True)

# 4. ç»Ÿè®¡æ”¶é›†
collector = CacheStatsCollector(analyzer)
stats = collector.collect()
collector.print_summary(stats, detailed=True)

# å¯¼å‡ºç»Ÿè®¡
collector.export_stats(stats, 'cache_stats.json')
```

### æ¸…ç†ç­–ç•¥

| ç­–ç•¥ | è¯´æ˜ |
|------|------|
| `LRU` | æŒ‰åˆ›å»ºæ—¶é—´æ’åºï¼Œåˆ é™¤æœ€æ—§çš„ |
| `OLDEST` | æœ€æ—§çš„ä¼˜å…ˆ |
| `LARGEST` | æœ€å¤§çš„ä¼˜å…ˆ |
| `VERSION_MISMATCH` | æ’ä»¶ç‰ˆæœ¬ä¸åŒ¹é…çš„ |
| `FAILED_INTEGRITY` | å®Œæ•´æ€§æ£€æŸ¥å¤±è´¥çš„ |
| `BY_RUN` | æŒ‰è¿è¡Œæ¸…ç† |
| `BY_DATA_TYPE` | æŒ‰æ•°æ®ç±»å‹æ¸…ç† |

### CLI å‘½ä»¤

```bash
# ç¼“å­˜æ¦‚è§ˆ
waveform-cache info [--run RUN_ID] [--detailed] [--storage-dir PATH]

# è¯¦ç»†ç»Ÿè®¡
waveform-cache stats [--run RUN_ID] [--detailed] [--export stats.json]

# è¯Šæ–­é—®é¢˜
waveform-cache diagnose [--run RUN_ID] [--fix] [--dry-run]

# åˆ—å‡ºç¼“å­˜æ¡ç›®
waveform-cache list [--run RUN_ID] [--data-type TYPE] [--min-size BYTES]

# æ¸…ç†ç¼“å­˜
waveform-cache clean --strategy lru --size-mb 500 [--dry-run]
waveform-cache clean --strategy oldest --days 30 [--no-dry-run]
waveform-cache clean --strategy largest --max-entries 10 --dry-run
```

### è¯Šæ–­é—®é¢˜ç±»å‹

| ç±»å‹ | ä¸¥é‡æ€§ | è¯´æ˜ |
|------|--------|------|
| `VERSION_MISMATCH` | warning | æ’ä»¶ç‰ˆæœ¬ä¸ç¼“å­˜ä¸åŒ¹é… |
| `MISSING_METADATA` | error | å…ƒæ•°æ®æ–‡ä»¶ç¼ºå¤± |
| `MISSING_DATA_FILE` | error | æ•°æ®æ–‡ä»¶ç¼ºå¤± |
| `SIZE_MISMATCH` | error | æ–‡ä»¶å¤§å°ä¸åŒ¹é… |
| `CHECKSUM_FAILED` | error | æ ¡éªŒå’ŒéªŒè¯å¤±è´¥ |
| `ORPHAN_FILE` | warning | å­¤å„¿æ–‡ä»¶ï¼ˆæ— å…ƒæ•°æ®ï¼‰ |
| `STORAGE_VERSION_MISMATCH` | warning | å­˜å‚¨ç‰ˆæœ¬ä¸åŒ¹é… |

### å®‰å…¨ç‰¹æ€§

- **é»˜è®¤ dry-run**: æ‰€æœ‰æ¸…ç†å’Œä¿®å¤æ“ä½œé»˜è®¤ä¸ºæ¼”ç»ƒæ¨¡å¼
- **çº¿ç¨‹å®‰å…¨**: CacheAnalyzer ä½¿ç”¨é”ä¿æŠ¤ç¼“å­˜ç´¢å¼•
- **å¢é‡æ‰«æ**: æ”¯æŒå¢é‡æ‰«æé¿å…é‡å¤éå†
- **è¯¦ç»†é¢„è§ˆ**: æ‰§è¡Œå‰å¯é¢„è§ˆæ‰€æœ‰å°†è¦æ‰§è¡Œçš„æ“ä½œ

## Common Pitfalls

1. **Generator Exhaustion**: Generators can only be consumed once; repeat access triggers recomputation
2. **Missing run_id**: Always pass `run_id` to `Context.get_data()` to avoid data conflicts
3. **Cache Invalidation**: Bump `version` when changing plugin logic/dtype/options
4. **Data Paths**: Default data directory is `DAQ/<run_name>`; missing files cause `FileNotFoundError`
5. **Chunk Boundaries**: Record endtime must not exceed chunk boundary; validate with `check_chunk_boundaries()`
6. **Timestamp Index**: After modifying `st_waveforms`, call `_build_timestamp_index()` to rebuild index
7. **Waveform Access**: With `load_waveforms=False`, `get_waveform_at()` returns None
8. **Plugin dtype**: `output_dtype` can be either:
   - Valid NumPy dtype (e.g., `np.dtype([('time', '<f8'), ('charge', '<f4')])`)
   - Type annotation strings for non-array outputs (e.g., `"List[np.ndarray]"`, `"pd.DataFrame"`)
   - Framework automatically handles both cases in lineage tracking and validation

## Testing Notes

- Test script auto-activates conda environment `pyroot-kernel`
- If DAQ data files missing, tests will `pytest.skip()` gracefully
- Coverage report generated in `htmlcov/` directory
- Use `scripts/benchmark_io.py` to test I/O performance with different chunksizes

## File Structure Notes

- `waveform_analysis/core/`: Core processing logic (modular subdirectories since 2026-01)
  - `context.py`, `dataset.py`: Core files (root level)
  - `storage/`: Storage layer (memmap, backends, cache, compression, integrity)
    - `cache_analyzer.py`: ç¼“å­˜åˆ†æå™¨å’Œ CacheEntry æ•°æ®ç±»
    - `cache_diagnostics.py`: ç¼“å­˜è¯Šæ–­å’Œä¿®å¤å·¥å…·
    - `cache_cleaner.py`: æ™ºèƒ½ç¼“å­˜æ¸…ç†ç­–ç•¥
    - `cache_statistics.py`: ç¼“å­˜ç»Ÿè®¡æ”¶é›†å’ŒæŠ¥å‘Š
  - `execution/`: Execution layer (manager, config, timeout)
  - `plugins/`: Plugin system (æŒ‰åŠ é€Ÿå™¨åˆ’åˆ†æ¶æ„ï¼Œsince 2026-01)
    - `core/`: Plugin infrastructure (base, streaming, adapters, hot_reload, etc.)
    - `builtin/`: Built-in plugins organized by accelerator
      - `cpu/`: CPU implementations (NumPy/SciPy/Numba)
        - `standard.py`: 10 standard data processing plugins
        - `filtering.py`: FilteredWaveformsPlugin (Butterworth, Savitzky-Golay)
        - `peak_finding.py`: SignalPeaksPlugin (scipy.signal.find_peaks)
      - `jax/`: JAX GPU implementations (å¾…å¼€å‘ - Phase 2)
      - `streaming/`: Streaming plugins (å¾…å¼€å‘ - Phase 3)
        - `cpu/`: CPU streaming plugins
        - `jax/`: JAX streaming plugins
      - `legacy/`: Deprecated plugins for backward compatibility
        - `__init__.py`: Lazy import with deprecation warnings
        - `standard.py`: Original standard plugins
        - `signal_processing.py`: Original signal processing plugins
      - `streaming_examples.py`: Streaming plugin examples (å¾…è¿ç§»)
  - `processing/`: Data processing (loader, processor, analyzer, chunk)
  - `data/`: Data management (query, export)
  - `foundation/`: Framework basics (exceptions, mixins, model, utils, progress)
- `waveform_analysis/cli_cache.py`: ç¼“å­˜ç®¡ç† CLI å‘½ä»¤ (waveform-cache)
- `waveform_analysis/utils/`: Utilities (DAQ adapters, I/O, visualization)
- `waveform_analysis/fitting/`: Physics fitting models
- `tests/`: Unit and integration tests
  - `test_cache_analyzer.py`: CacheAnalyzer æµ‹è¯•
  - `test_cache_diagnostics.py`: CacheDiagnostics æµ‹è¯•
  - `test_cache_cleaner.py`: CacheCleaner æµ‹è¯•
  - `test_cache_statistics.py`: CacheStatsCollector æµ‹è¯•
- `examples/`: Usage demonstrations
- `docs/`: Architecture, guides, and implementation details
- `scripts/`: Helper scripts (testing, benchmarking)
- `DAQ/`: Data directory (not in package)
- `outputs/`: Results directory (not in package)

## Key Documentation Files

- `docs/ARCHITECTURE.md`: Complete architecture and data flow
- `docs/CACHE.md`: Lineage tracking and cache strategy
- `docs/STREAMING_GUIDE.md`: Streaming framework usage
- `docs/MEMORY_OPTIMIZATION.md`: Memory-saving techniques
- `docs/EXECUTOR_MANAGER_GUIDE.md`: Parallel execution management
- `docs/QUICKSTART.md`: Quick start examples
- `docs/PREVIEW_EXECUTION.md`: Preview execution plans before running
- `docs/SIGNAL_PROCESSING_PLUGINS.md`: Signal processing plugins (filtering, peak detection)
- **Lineage Visualization**: See `CLAUDE.md` Â§ Lineage Visualization for color-coded interactive graph features
- `.github/copilot-instructions.md`: Detailed development guidelines (Chinese)
