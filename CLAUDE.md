# CLAUDE.md

This file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.

## Project Overview

WaveformAnalysis is a Python package for processing and analyzing DAQ (Data Acquisition) system waveform data. It features a **plugin-based architecture** inspired by strax, with support for both static and streaming data processing, automatic caching with lineage tracking, and memory-optimized workflows.

**Key Characteristics:**
- Plugin-based processing with automatic dependency resolution (DAG)
- Context-managed stateless execution (explicit `run_id` required)
- Zero-copy caching via `numpy.memmap` with atomic writes
- Streaming support for memory-efficient processing of large datasets
- Global executor management for thread/process pool reuse

## Development Commands

### Installation
```bash
# Quick install (recommended)
./install.sh

# Manual install (development mode)
pip install -e .

# With development dependencies
pip install -e ".[dev]"
```

### Testing
```bash
# Run tests (auto-activates conda env pyroot-kernel)
./scripts/run_tests.sh

# Or via Makefile
make test

# Run tests with specific pytest args
./scripts/run_tests.sh -v -k test_name

# Custom conda environment
CONDA_ENV=my-env ./scripts/run_tests.sh
```

### Benchmarking
```bash
# Run I/O benchmark
make bench

# Custom benchmark parameters
python scripts/benchmark_io.py --n-files 100 --n-channels 2 --n-samples 500 --reps 3
```

### Code Quality
```bash
# Format code (Black)
black waveform_analysis/ --line-length 100

# Type checking
mypy waveform_analysis/

# Run tests with coverage
pytest -v --cov=waveform_analysis --cov-report=html
```

### CLI Usage
```bash
# Process a run
waveform-process --run-name 50V_OV_circulation_20thr --verbose

# Scan DAQ directory
waveform-process --scan-daq --daq-root DAQ

# Show DAQ overview
waveform-process --show-daq --daq-root DAQ
```

## Configuration Management

WaveformAnalysis æä¾›çµæ´»çš„é…ç½®ç³»ç»Ÿï¼Œæ”¯æŒå…¨å±€é…ç½®å’Œæ’ä»¶ç‰¹å®šé…ç½®ã€‚

### æŸ¥çœ‹æ’ä»¶é…ç½®é€‰é¡¹

```python
# åˆ—å‡ºæ‰€æœ‰æ’ä»¶çš„é…ç½®é€‰é¡¹
ctx.list_plugin_configs()

# åªæŸ¥çœ‹ç‰¹å®šæ’ä»¶çš„é…ç½®
ctx.list_plugin_configs(plugin_name='waveforms')

# è·å–é…ç½®å­—å…¸è€Œä¸æ‰“å°ï¼ˆç”¨äºç¨‹åºåŒ–å¤„ç†ï¼‰
config_info = ctx.list_plugin_configs(verbose=False)
```

`list_plugin_configs()` åŠŸèƒ½ç‰¹æ€§ï¼š
- ğŸ“¦ æ˜¾ç¤ºæ‰€æœ‰æ’ä»¶çš„é…ç½®é€‰é¡¹ã€é»˜è®¤å€¼ã€ç±»å‹å’Œå¸®åŠ©æ–‡æœ¬
- âœ“/âš™ï¸ å›¾æ ‡åŒºåˆ†é»˜è®¤å€¼å’Œå·²ä¿®æ”¹çš„é…ç½®
- ğŸ”§ æ˜ç¡®æ ‡è®°å·²è‡ªå®šä¹‰çš„é…ç½®å€¼
- ğŸ“Š ç»Ÿè®¡å·²æ³¨å†Œæ’ä»¶æ•°ã€é…ç½®é€‰é¡¹æ€»æ•°å’Œå·²ä¿®æ”¹é…ç½®æ•°
- ğŸ“ è‡ªåŠ¨æ¢è¡Œå¤„ç†é•¿æè¿°å’Œå¸®åŠ©æ–‡æœ¬
- ğŸ¨ æ¸…æ™°çš„è¡¨æ ¼è¾¹æ¡†å’Œå±‚æ¬¡ç»“æ„

### æŸ¥çœ‹å½“å‰é…ç½®å€¼

```python
# æ˜¾ç¤ºå…¨å±€é…ç½®ï¼ˆåŒ…å«é…ç½®é¡¹ä½¿ç”¨æƒ…å†µï¼‰
ctx.show_config()

# æ˜¾ç¤ºç‰¹å®šæ’ä»¶çš„è¯¦ç»†é…ç½®
ctx.show_config('waveforms')

# æ˜¾ç¤ºå…¨å±€é…ç½®ä½†ä¸æ˜¾ç¤ºä½¿ç”¨æƒ…å†µ
ctx.show_config(show_usage=False)
```

`show_config()` å¢å¼ºåŠŸèƒ½ç‰¹æ€§ï¼š
- ğŸ” **æ™ºèƒ½åˆ†æé…ç½®é¡¹ä½¿ç”¨æƒ…å†µ** - è‡ªåŠ¨è¯†åˆ«å“ªäº›æ’ä»¶ä½¿ç”¨äº†æ¯ä¸ªå…¨å±€é…ç½®é¡¹
- ğŸ“‚ **ä¸‰ç±»é…ç½®åˆ†ç»„æ˜¾ç¤º**ï¼š
  - **å…¨å±€é…ç½®é¡¹** - è¢«æ’ä»¶ä½¿ç”¨çš„é…ç½®ï¼Œæ˜¾ç¤ºä½¿ç”¨æ’ä»¶åˆ—è¡¨
  - **æ’ä»¶ç‰¹å®šé…ç½®** - ä»…å¯¹å•ä¸ªæ’ä»¶ç”Ÿæ•ˆçš„é…ç½®ï¼ˆåµŒå¥—å­—å…¸æˆ–ç‚¹åˆ†éš”ï¼‰
  - **æœªä½¿ç”¨é…ç½®** - æœªè¢«ä»»ä½•æ’ä»¶ä½¿ç”¨çš„é…ç½®é¡¹ï¼ˆå¸®åŠ©å‘ç°é…ç½®é”™è¯¯ï¼‰
- âš™ï¸ **è¯¦ç»†çš„æ’ä»¶é…ç½®è§†å›¾** - æŸ¥çœ‹ç‰¹å®šæ’ä»¶æ—¶æ˜¾ç¤ºå®Œæ•´ä¿¡æ¯ï¼š
  - é…ç½®å€¼ä¸é»˜è®¤å€¼å¯¹æ¯”
  - é…ç½®é¡¹ç±»å‹å’Œè¯´æ˜
  - è‡ªå®šä¹‰çŠ¶æ€æ ‡è®°
- ğŸ“Š **ç»Ÿè®¡æ¦‚è§ˆ** - ä¸€ç›®äº†ç„¶åœ°çœ‹åˆ°é…ç½®é¡¹åˆ†å¸ƒæƒ…å†µ

### è®¾ç½®é…ç½®

```python
# å…¨å±€é…ç½®
ctx.set_config({'n_channels': 2, 'threshold': 50})

# æ’ä»¶ç‰¹å®šé…ç½®ï¼ˆæ¨èï¼Œé¿å…å†²çªï¼‰
ctx.set_config({'threshold': 50}, plugin_name='peaks')

# æŸ¥çœ‹å½“å‰é…ç½®å€¼
ctx.show_config('plugin_name')
```

### é…ç½®è§£æç³»ç»Ÿ (ConfigResolver) [NEW - 2026-01]

WaveformAnalysis æä¾›ç»Ÿä¸€çš„é…ç½®è§£æç³»ç»Ÿï¼Œæ”¯æŒå¤šçº§é…ç½®æ¥æºå’Œ adapter æ¨æ–­ã€‚

#### é…ç½®ä¼˜å…ˆçº§

é…ç½®åˆå¹¶ä¼˜å…ˆçº§ï¼ˆä»é«˜åˆ°ä½ï¼‰ï¼š
1. **æ˜¾å¼é…ç½®** - ç”¨æˆ·é€šè¿‡ `set_config()` è®¾ç½®
2. **Adapter æ¨æ–­** - ä» DAQ adapter è‡ªåŠ¨æ¨æ–­ï¼ˆå¦‚é‡‡æ ·ç‡ã€æ—¶é—´é—´éš”ï¼‰
3. **æ’ä»¶é»˜è®¤å€¼** - `plugin.options[key].default`

#### æŸ¥çœ‹è§£æåçš„é…ç½®

```python
# è·å–è§£æåçš„é…ç½®ï¼ˆåŒ…å«æ¥æºä¿¡æ¯ï¼‰
resolved = ctx.get_resolved_config('peaks', adapter_name='vx2730')
print(resolved.summary(verbose=True))

# æ˜¾ç¤ºé…ç½®åŠæ¥æº
ctx.show_resolved_config('peaks')
```

#### å¯æ¨æ–­çš„é…ç½®é¡¹

ä»¥ä¸‹é…ç½®é¡¹å¯ä» DAQ adapter è‡ªåŠ¨æ¨æ–­ï¼š

| é…ç½®é¡¹ | è¯´æ˜ | ç¤ºä¾‹å€¼ (VX2730) |
|--------|------|-----------------|
| `sampling_rate_hz` | é‡‡æ ·ç‡ (Hz) | 500000000.0 |
| `sampling_rate` / `fs` | é‡‡æ ·ç‡ (GHz) | 0.5 |
| `dt_ns` / `sampling_interval_ns` | é‡‡æ ·é—´éš” (ns) | 2.0 |
| `dt_ps` | é‡‡æ ·é—´éš” (ps) | 2000.0 |
| `timestamp_unit` | æ—¶é—´æˆ³å•ä½ | 'ps' |
| `expected_samples` | é¢„æœŸé‡‡æ ·ç‚¹æ•° | 1030 |

#### å…¼å®¹å±‚ç®¡ç† (CompatManager)

å¤„ç†å‚æ•°åˆ«åå’Œå¼ƒç”¨è­¦å‘Šï¼š

```python
from waveform_analysis.core.config import CompatManager, DeprecationInfo

# æ³¨å†Œå‚æ•°åˆ«å
CompatManager.register_alias('old_param', 'new_param', plugin_name='peaks')

# æ³¨å†Œå¼ƒç”¨ä¿¡æ¯
CompatManager.register_deprecation(DeprecationInfo(
    old_name='break_threshold_ns',
    new_name='break_threshold_ps',
    deprecated_in='1.1.0',
    removed_in='2.0.0'
))

# æŸ¥çœ‹å…¼å®¹å±‚æ‘˜è¦
manager = CompatManager()
print(manager.summary())
```

**å¼ƒç”¨è¿‡æœŸæ£€æŸ¥**ï¼šå½“å‰ç‰ˆæœ¬ >= `removed_in` æ—¶ï¼Œä½¿ç”¨å·²å¼ƒç”¨å‚æ•°ä¼šæŠ›å‡º `ValueError`ã€‚

## Architecture Overview

### Core Structure (Modular Subdirectories)

ä» 2026-01 ç‰ˆæœ¬å¼€å§‹ï¼Œ`core/` ç›®å½•é‡‡ç”¨**æ¨¡å—åŒ–å­ç›®å½•æ¶æ„**ï¼Œå°†åŸæœ¬æ‰å¹³çš„ 27 ä¸ªæ–‡ä»¶é‡æ„ä¸º 6 ä¸ªåŠŸèƒ½å­ç›®å½•ï¼š

- **`storage/`**: å­˜å‚¨å±‚ï¼ˆmemmap, backends, cache, compression, integrityï¼‰
- **`execution/`**: æ‰§è¡Œå±‚ï¼ˆmanager, config, timeoutï¼‰
- **`plugins/`**: æ’ä»¶ç³»ç»Ÿï¼ˆåˆ†ç¦» core/ å’Œ builtin/ï¼‰
- **`processing/`**: æ•°æ®å¤„ç†ï¼ˆloader, processor, analyzer, chunkï¼‰
- **`data/`**: æ•°æ®ç®¡ç†ï¼ˆquery, exportï¼‰
- **`foundation/`**: æ¡†æ¶åŸºç¡€ï¼ˆexceptions, mixins, model, utils, progressï¼‰

æ ¸å¿ƒæ–‡ä»¶ `context.py` å’Œ `dataset.py` ä¿æŒåœ¨ core/ æ ¹ç›®å½•ã€‚

æ–°å¢æ¨¡å—ï¼š
- **`config/`**: é…ç½®ç³»ç»Ÿï¼ˆresolver, compat, adapter_info, typesï¼‰

### Core Components

1. **Context Layer** (`core/context.py`)
   - Central coordinator managing plugins, configuration, and caching
   - **Stateless**: All operations require explicit `run_id` parameter
   - Data stored in `_results[(run_id, data_name)]`
   - Automatic dependency resolution with cycle detection
   - Lineage-based caching with SHA1 hashing of plugin code, version, config, dtype

2. **Plugin System** (`core/plugins/`)
   - **Modular**: Core infrastructure (`plugins/core/`) ä¸å†…ç½®æ’ä»¶ï¼ˆ`plugins/builtin/`ï¼‰åˆ†ç¦»
   - **Accelerator-based Architecture** (since 2026-01): æŒ‰åŠ é€Ÿå™¨åˆ’åˆ†æ’ä»¶
     - `builtin/cpu/`: CPU å®ç°ï¼ˆNumPy/SciPy/Numbaï¼‰
     - `builtin/jax/`: JAX GPU å®ç°ï¼ˆå¾…å¼€å‘ï¼‰
     - `builtin/streaming/`: æµå¼å¤„ç†æ’ä»¶ï¼ˆCPU å·²æœ‰ SignalPeaksStreamPluginï¼Œå…¶å®ƒå¾…å¼€å‘ï¼‰
     - `builtin/legacy/`: å‘åå…¼å®¹å±‚ï¼ˆå¼ƒç”¨è­¦å‘Šï¼‰
   - Each plugin declares: `provides`, `depends_on`, `options`, `version`, `dtype`
   - `compute()` returns ndarray or generator
   - `is_side_effect=True` isolates outputs to `_side_effects/{run_id}/{plugin_name}`
   - **CPU Standard Plugins**:
     - Data processing: RawFiles â†’ Waveforms â†’ StWaveforms â†’ Features â†’ DataFrame â†’ GroupedEvents â†’ PairedEvents
     - Signal processing: FilteredWaveforms (Butterworth/Savitzky-Golay), SignalPeaks (scipy.signal)
   - **Plugin Organization**:
     - `cpu/standard.py`: 10ä¸ªæ ‡å‡†æ•°æ®å¤„ç†æ’ä»¶
     - `cpu/filtering.py`: FilteredWaveformsPlugin
     - `cpu/peak_finding.py`: SignalPeaksPlugin

3. **Storage Layer** (`core/storage/`)
   - `MemmapStorage` (`storage/memmap.py`): Zero-copy array persistence with atomic writes (`.tmp` â†’ rename)
   - `StorageBackend` (`storage/backends.py`): Pluggable backends (SQLite, etc.)
   - `CacheManager` (`storage/cache.py`): Lineage-based cache validation
   - `CompressionManager` (`storage/compression.py`): Blosc2, LZ4, Zstd, Gzip
   - `IntegrityChecker` (`storage/integrity.py`): Checksum validation
   - Validates `dtype.descr` and `STORAGE_VERSION`
   - File locking for concurrent access protection
   - Watch signature (`WATCH_SIG_KEY`) tracks input file mtime/size for cache invalidation

4. **Streaming Framework** (`core/plugins/core/streaming.py`, `core/plugins/builtin/streaming/`)
   - `StreamingPlugin`: Returns chunk iterators instead of static data
   - `StreamingContext`: Manages chunk flows with automatic parallelization
   - Time-aligned chunk processing with boundary validation
   - Mixed static/streaming plugin support

5. **Executor Management** (`core/execution/`)
   - `ExecutorManager` (`execution/manager.py`): Global singleton for thread/process pool reuse
   - `EXECUTOR_CONFIGS` (`execution/config.py`): Predefined configs: `io_intensive`, `cpu_intensive`, `large_data`, `small_data`
   - `TimeoutManager` (`execution/timeout.py`): Timeout control
   - Context manager support: `with get_executor('io_intensive') as executor:`
   - Helper functions: `parallel_map()`, `parallel_apply()`

6. **Dataset API** (`core/dataset.py`)
   - High-level chainable interface wrapping Context
   - Memory optimization: `load_waveforms=False` skips waveform extraction (saves 70-80% memory)
   - Feature registration system
   - Timestamp indexing for fast `get_waveform_at()` lookups

7. **Chunk Utilities** (`core/processing/chunk.py`)
   - `Chunk(data, start, end, run_id, ...)`: Encapsulates data with time boundaries
   - Time range operations: `select_time_range()`, `clip_to_time_range()`
   - Validation: `check_monotonic()`, `check_no_overlap()`, `check_chunk_boundaries()`
   - Splitting/merging: `split_by_time()`, `merge_chunks()`, `rechunk()`

8. **Time Range Query** (`core/data/query.py`) [NEW - Phase 2.2]
   - `TimeIndex`: Efficient time indexing with O(log n) binary search queries
   - `TimeRangeQueryEngine`: Manages multiple data type indices
   - Context integration: `time_range()`, `clear_time_index()`
   - Query result caching for repeated queries
   - Example: `ctx.time_range('run_001', 'st_waveforms', start_time=1000, end_time=2000)`

9. **Strax Plugin Adapter** (`core/plugins/core/adapters.py`) [NEW - Phase 2.3]
   - `StraxPluginAdapter`: Wraps strax plugins for seamless integration
   - `StraxContextAdapter`: Provides strax-style API (`get_array`, `get_df`, `search_field`)
   - Automatic metadata extraction and parameter mapping
   - Configuration option compatibility
   - Example: `adapter = wrap_strax_plugin(MyStraxPlugin); ctx.register_plugin(adapter)`

10. **Batch Processing & Export** (`core/data/export.py`) [NEW - Phase 3.1 & 3.2]
    - `BatchProcessor`: Parallel/serial processing of multiple runs
    - `DataExporter`: Unified export interface (Parquet, HDF5, CSV, JSON, NumPy)
    - Progress tracking and flexible error handling
    - Example: `processor.process_runs(run_ids, 'peaks', max_workers=4)`
    - Example: `exporter.export(data, 'output.parquet')`

11. **Hot Reload** (`core/plugins/core/hot_reload.py`) [NEW - Phase 3.3]
    - `PluginHotReloader`: File change monitoring and automatic module reloading
    - Cache consistency maintenance after reload
    - Auto-reload daemon thread support
    - Example: `reloader = enable_hot_reload(ctx, ['my_plugin'], auto_reload=True)`

12. **WaveformStruct DAQ Decoupling** (`core/processing/processor.py`) [NEW - 2026-01]
    - `WaveformStructConfig`: Configuration class for DAQ format specifications
    - `WaveformStruct`: Decoupled from VX2730 hardware, supports multiple DAQ formats
    - Dynamic `RECORD_DTYPE` creation based on actual waveform length
    - Column mapping from `FormatSpec` (board, channel, timestamp, samples_start, baseline_start/end)
    - **Backward Compatible**: No-config usage defaults to VX2730 format
    - **Usage Modes**:
      - Default (VX2730): `WaveformStruct(waveforms)`
      - From adapter: `WaveformStruct.from_adapter(waveforms, "vx2730")`
      - Custom config: `WaveformStruct(waveforms, config=WaveformStructConfig(format_spec=custom_spec))`
    - **Plugin Integration**: `StWaveformsPlugin` supports `daq_adapter` option
      - Example: `ctx.set_config({'daq_adapter': 'vx2730'}, plugin_name='st_waveforms')`

13. **Time Field Unification** (`core/processing/processor.py`) [NEW - 2026-01]
    - **RECORD_DTYPE** now includes both `time` and `timestamp` fields:
      - `time` (i8): Absolute system time (Unix timestamp, nanoseconds)
      - `timestamp` (i8): ADC raw timestamp (picoseconds, unified to ps)
    - **Time Conversion**: `time = epoch_ns + timestamp // 1000`
    - **Epoch Source**: Automatically obtained from file creation time via `DAQAdapter.get_file_epoch()`
    - **Backward Compatible**: Without epoch, falls back to relative time (`time = timestamp // 1000`)
    - **Auto-effective**: `chunk.py` and `query.py` prioritize `time` by default; `streaming.py` defaults to `timestamp` (ps)
    - **Usage**: Set `daq_adapter` to enable automatic epoch extraction
      - Example: `ctx.set_config({'daq_adapter': 'vx2730'})`

14. **Config System** (`core/config/`) [NEW - 2026-01]
    - `ConfigResolver`: ç»Ÿä¸€é…ç½®è§£æï¼Œæ”¯æŒå¤šçº§æ¥æºï¼ˆæ˜¾å¼ > adapteræ¨æ–­ > æ’ä»¶é»˜è®¤ï¼‰
    - `CompatManager`: å…¼å®¹å±‚ç®¡ç†ï¼Œå¤„ç†å‚æ•°åˆ«åå’Œå¼ƒç”¨è­¦å‘Š
    - `AdapterInfo`: DAQ adapter ä¿¡æ¯ï¼Œç”¨äºé…ç½®æ¨æ–­
    - `ResolvedConfig`: è§£æåçš„é…ç½®é›†åˆï¼ŒåŒ…å«æ¥æºè¿½è¸ª
    - **å¼ƒç”¨è¿‡æœŸæ£€æŸ¥**: å½“å‰ç‰ˆæœ¬ >= `removed_in` æ—¶æŠ›å‡º `ValueError`
    - Context é›†æˆ: `get_resolved_config()`, `show_resolved_config()`, `get_adapter_info()`

15. **Plugin Specification** (`core/plugins/core/spec.py`) [NEW - 2026-01]
    - `PluginSpec`: æ’ä»¶å®Œæ•´å¥‘çº¦è§„èŒƒï¼Œç”¨äºæ³¨å†Œæ ¡éªŒã€æ–‡æ¡£ç”Ÿæˆã€lineage hash
    - `ConfigField`: å£°æ˜å¼é…ç½®è§„èŒƒï¼Œä¸ `Option`ï¼ˆè¿è¡Œæ—¶éªŒè¯ï¼‰äº’è¡¥
    - `OutputSchema`: è¾“å‡ºæ•°æ® schemaï¼Œæ”¯æŒä» dtype è‡ªåŠ¨åˆ›å»º
    - `InputRequirement`: è¾“å…¥ä¾èµ–éœ€æ±‚ï¼Œæ”¯æŒç‰ˆæœ¬çº¦æŸ
    - `Capabilities`: æ’ä»¶èƒ½åŠ›å£°æ˜ï¼ˆstreaming, parallel, GPU, idempotentï¼‰
    - **è‡ªåŠ¨æå–**: `PluginSpec.from_plugin(plugin)` ä»æ’ä»¶å®ä¾‹è‡ªåŠ¨ç”Ÿæˆè§„èŒƒ
    - **ä¸¥æ ¼æ ¡éªŒ**: `ctx.register(plugin, require_spec=True)` å¯ç”¨æ³¨å†Œæ—¶æ ¡éªŒ

### Data Flow (Standard Pipeline)

```
CSV Files â†’ RawFilesPlugin â†’ WaveformsPlugin â†’ StWaveformsPlugin
                                                       â†“
                                              â”Œâ”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”
                                              â†“               â†“
                                        PeaksPlugin    ChargesPlugin
                                              â†“               â†“
                                              â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                                                      â†“
                                               DataFramePlugin
                                                      â†“
                                            GroupedEventsPlugin
                                               (Numba + MP)
                                                      â†“
                                            PairedEventsPlugin
```

## Important Conventions

### Architecture & Responsibility Separation
- **Context**: Only manages plugin DAG, config, lineage, and caching
- **Dataset**: Provides chainable interface and result access (delegates to Context)
- **Never** maintain parallel state in Dataset; map attributes (like `self.char` â†’ Context) to ensure single source of truth
- **Plugin Responsibility**: Each plugin does ONE thing; add new features as new plugins, not by expanding existing ones

### Run Identification
- **Always use `run_name`** instead of `char` (legacy term being phased out)
- Pass `run_id` explicitly to all `Context.get_data()` calls
- Missing `run_id` causes data overwrites and lineage conflicts

### Module Exports (`core/foundation/utils.py`)
All new modules **must** use the `exporter()` pattern:

```python
from waveform_analysis.core.foundation.utils import exporter
export, __all__ = exporter()

@export
class MyClass: ...

@export(name="AlternativeName")
def my_func(): ...

MY_CONST = export(42, name="MY_CONST")
```

### Naming Conventions
- Classes: `PascalCase`
- Functions/variables: `snake_case`
- Constants: `UPPER_SNAKE_CASE`
- Terminology: Use consistent business terms (waveforms/events/hits/chunks) across code and docs
- Event size: Use `event_length`, not `pair_len` or `pair_length`

### Cache Management
- Step-level cache: `set_step_cache(step, enabled=True, attrs=[...], persist_path=..., watch_attrs=[...])`
- Persistent cache writes `WATCH_SIG_KEY="__watch_sig__"` (mtime/size SHA1) for validation
- Cache automatically invalidates on plugin version/config/dtype changes
- Generator outputs consumed once; re-access triggers recomputation

### DAQ Adapter Configuration
- **Consistent Adapter Usage**: Use the same `daq_adapter` across the entire data flow (RawFiles â†’ Waveforms â†’ StWaveforms)
- **Global vs Plugin-Specific**: Prefer global config `ctx.set_config({'daq_adapter': 'vx2730'})` for consistency
- **Backward Compatibility**: Omitting `daq_adapter` defaults to VX2730 format
- **Custom Formats**: Create custom `FormatSpec` and register via `register_adapter()` for new DAQ systems
- **Plugin Support**: `RawFilesPlugin`, `WaveformsPlugin`, and `StWaveformsPlugin` all support `daq_adapter` option

### Time & Chunk Operations
- Records have `time`, `dt`, `length` fields; `endtime = time + dt * length`
- Chunk boundaries must be respected: record endtime â‰¤ chunk end
- Use `Chunk` objects for time-aligned data processing
- Validate with `check_sorted_by_time()` and `check_chunk_boundaries()`

### Performance Optimization
- **Numba JIT**: Available for hot loops (e.g., `group_multi_channel_hits` with `use_numba=True`)
- **Multiprocessing**: Use for large-scale CPU-bound tasks
- **Vectorization**: Prefer NumPy broadcasting over explicit loops
- **IO parallelization**: Use `ExecutorManager` with `io_intensive` config
- **CPU parallelization**: Use `ExecutorManager` with `cpu_intensive` config

## Plugin Architecture and Import Guide

### Accelerator-Based Plugin Organization (Since 2026-01)

æ’ä»¶æŒ‰ç…§è®¡ç®—åŠ é€Ÿå™¨ç±»å‹ç»„ç»‡ï¼Œæ”¯æŒ CPUã€JAXï¼ˆGPUï¼‰å’Œæµå¼å¤„ç†ï¼š

```
builtin/
â”œâ”€â”€ cpu/              # CPU å®ç° (NumPy/SciPy/Numba)
â”‚   â”œâ”€â”€ standard.py   # æ ‡å‡†æ•°æ®å¤„ç†æ’ä»¶
â”‚   â”œâ”€â”€ filtering.py  # æ»¤æ³¢æ’ä»¶
â”‚   â””â”€â”€ peak_finding.py # å¯»å³°æ’ä»¶
â”œâ”€â”€ jax/              # JAX GPU å®ç°ï¼ˆå¾…å¼€å‘ï¼‰
â”œâ”€â”€ streaming/        # æµå¼å¤„ç†æ’ä»¶ï¼ˆCPU å·²æœ‰ SignalPeaksStreamPluginï¼‰
â””â”€â”€ legacy/           # å‘åå…¼å®¹ï¼ˆå¼ƒç”¨ï¼‰
```

### Plugin Import Methods

```python
# æ–¹æ³• 1: ä» cpu/ ç›´æ¥å¯¼å…¥ï¼ˆæ¨èï¼Œæ˜ç¡®æŒ‡å®šåŠ é€Ÿå™¨ï¼‰
from waveform_analysis.core.plugins.builtin.cpu import (
    RawFilesPlugin,
    WaveformsPlugin,
    FilteredWaveformsPlugin,
    SignalPeaksPlugin,
)

# æ–¹æ³• 2: ä» builtin/ å¯¼å…¥ï¼ˆå‘åå…¼å®¹ï¼Œé»˜è®¤ä½¿ç”¨ CPU å®ç°ï¼‰
from waveform_analysis.core.plugins.builtin.cpu import (
    RawFilesPlugin,
    WaveformsPlugin,
    FilteredWaveformsPlugin,
    SignalPeaksPlugin,
)

# æ–¹æ³• 3: ä» legacy/ å¯¼å…¥ï¼ˆä¸æ¨èï¼Œä¼šå‘å‡ºå¼ƒç”¨è­¦å‘Šï¼‰
from waveform_analysis.core.plugins.builtin.cpu import RawFilesPlugin
# DeprecationWarning: RawFilesPlugin å·²è¢«å¼ƒç”¨ï¼Œå°†åœ¨ä¸‹ä¸€ä¸ªä¸»ç‰ˆæœ¬ä¸­ç§»é™¤...
```

### Available CPU Plugins

#### æ ‡å‡†æ•°æ®å¤„ç†æ’ä»¶ (`cpu/standard.py`)
- `RawFilesPlugin`: æ‰«æå’Œåˆ†ç»„åŸå§‹ CSV æ–‡ä»¶
  - æ”¯æŒ `daq_adapter` é€‰é¡¹æŒ‡å®š DAQ æ ¼å¼ï¼ˆå¦‚ 'vx2730'ï¼‰
  - é›†æˆ DAQ å…ƒæ•°æ®æ‰«æåŠŸèƒ½
- `WaveformsPlugin`: æå–æ³¢å½¢æ•°æ®
  - æ”¯æŒ `daq_adapter` é€‰é¡¹ç”¨äºæ ¼å¼ç‰¹å®šçš„æ•°æ®åŠ è½½
  - æ”¯æŒå¤šè¿›ç¨‹å¹¶è¡ŒåŠ è½½å’Œè‡ªå®šä¹‰æ‰§è¡Œå™¨
- `StWaveformsPlugin`: ç»“æ„åŒ–æ³¢å½¢æ•°ç»„
  - æ”¯æŒ `daq_adapter` é€‰é¡¹ï¼Œé€šè¿‡ `WaveformStructConfig` è§£è€¦ DAQ æ ¼å¼
  - å‘åå…¼å®¹ï¼šé»˜è®¤ä½¿ç”¨ VX2730 é…ç½®
- `HitFinderPlugin`: æ£€æµ‹ Hit äº‹ä»¶
- `PeaksPlugin`: å³°å€¼ç‰¹å¾è®¡ç®—ï¼ˆç›´æ¥ä¾èµ– st_waveformsï¼‰
- `ChargesPlugin`: ç”µè·ç§¯åˆ†è®¡ç®—ï¼ˆç›´æ¥ä¾èµ– st_waveformsï¼‰
- `DataFramePlugin`: æ„å»º DataFrame
- `GroupedEventsPlugin`: æ—¶é—´çª—å£åˆ†ç»„ï¼ˆæ”¯æŒ Numba åŠ é€Ÿï¼‰
- `PairedEventsPlugin`: è·¨é€šé“äº‹ä»¶é…å¯¹

#### ä¿¡å·å¤„ç†æ’ä»¶
- `FilteredWaveformsPlugin` (`cpu/filtering.py`): æ³¢å½¢æ»¤æ³¢
  - Butterworth å¸¦é€šæ»¤æ³¢å™¨
  - Savitzky-Golay æ»¤æ³¢å™¨
- `SignalPeaksPlugin` (`cpu/peak_finding.py`): é«˜çº§å³°å€¼æ£€æµ‹
  - åŸºäº scipy.signal.find_peaks
  - æ”¯æŒå¯¼æ•°æ£€æµ‹ã€é«˜åº¦ã€è·ç¦»ã€æ˜¾è‘—æ€§ç­‰å‚æ•°
  - è¿”å› `ADVANCED_PEAK_DTYPE` ç»“æ„åŒ–æ•°ç»„

### Migration Guide from Legacy Plugins

å¦‚æœä½ çš„ä»£ç ä½¿ç”¨äº†æ—§çš„å¯¼å…¥æ–¹å¼ï¼Œå»ºè®®è¿ç§»åˆ°æ–°æ¶æ„ï¼š

```python
# æ—§æ–¹å¼ï¼ˆä¼šå‘å‡ºè­¦å‘Šï¼‰
from waveform_analysis.core.plugins.builtin.cpu import RawFilesPlugin
from waveform_analysis.core.plugins.builtin.cpu import FilteredWaveformsPlugin

# æ–°æ–¹å¼ï¼ˆæ¨èï¼‰
from waveform_analysis.core.plugins.builtin.cpu import RawFilesPlugin, FilteredWaveformsPlugin
```

## Common Patterns

### Basic Dataset Usage
```python
from waveform_analysis import WaveformDataset

ds = WaveformDataset(run_name="50V_OV_circulation_20thr", n_channels=2)
(ds
    .load_raw_data()
    .extract_waveforms()
    .structure_waveforms()
    .build_waveform_features()
    .build_dataframe()
    .group_events(time_window_ns=100)
    .pair_events())

df = ds.get_paired_events()
```

### Memory-Optimized Workflow
```python
# Skip waveform extraction (saves 70-80% memory)
ds = WaveformDataset(run_name="...", load_waveforms=False)
ds.load_raw_data().extract_waveforms().structure_waveforms()  # All skipped
ds.build_waveform_features()  # Still computes features
ds.get_waveform_at(0)  # Returns None with warning
```

### Plugin-Based Context Usage
```python
from waveform_analysis.core.context import Context
from waveform_analysis.core.standard_plugins import *

# æ¨èï¼šä½¿ç”¨ data_root é…ç½®ï¼Œç¼“å­˜å°†å­˜å‚¨åœ¨ DAQ/{run_id}/_cache/
ctx = Context(config={"data_root": "DAQ", "n_channels": 2})
ctx.register_plugin(RawFilesPlugin())
ctx.register_plugin(WaveformsPlugin())

# æˆ–è€…æ˜¾å¼æŒ‡å®šå­˜å‚¨ç›®å½•
# ctx = Context(storage_dir="./strax_data")

hits = ctx.get_data("run_001", "hits")  # Auto-resolves dependencies
```

### Preview Execution (è¿è¡Œå‰ç¡®è®¤ Lineage)
```python
# åœ¨å®é™…æ‰§è¡Œå‰é¢„è§ˆæ‰§è¡Œè®¡åˆ’
ctx.preview_execution('run_001', 'signal_peaks')

# è¾“å‡ºåŒ…å«ï¼š
# - æ‰§è¡Œè®¡åˆ’ï¼ˆæ’ä»¶æ‰§è¡Œé¡ºåºï¼‰
# - ä¾èµ–å…³ç³»æ ‘
# - è‡ªå®šä¹‰é…ç½®å‚æ•°ï¼ˆä»…æ˜¾ç¤ºéé»˜è®¤å€¼ï¼‰
# - ç¼“å­˜çŠ¶æ€ï¼ˆå“ªäº›å·²ç¼“å­˜ï¼Œå“ªäº›éœ€è¦è®¡ç®—ï¼‰

# ç¨‹åºåŒ–ä½¿ç”¨é¢„è§ˆç»“æœ
result = ctx.preview_execution('run_001', 'signal_peaks')
needs_compute = [p for p, s in result['cache_status'].items() if s['needs_compute']]
print(f"éœ€è¦è®¡ç®— {len(needs_compute)} ä¸ªæ’ä»¶")

# ç¡®è®¤åæ‰§è¡Œ
data = ctx.get_data('run_001', 'signal_peaks')

# ä¸åŒè¯¦ç»†ç¨‹åº¦
ctx.preview_execution('run_001', 'signal_peaks', verbose=0)  # ç®€æ´
ctx.preview_execution('run_001', 'signal_peaks', verbose=1)  # æ ‡å‡†ï¼ˆé»˜è®¤ï¼‰
ctx.preview_execution('run_001', 'signal_peaks', verbose=2)  # è¯¦ç»†

# é€‰æ‹©æ€§æ˜¾ç¤º
ctx.preview_execution('run_001', 'signal_peaks',
                      show_tree=False,   # ä¸æ˜¾ç¤ºä¾èµ–æ ‘
                      show_config=True,  # æ˜¾ç¤ºé…ç½®
                      show_cache=True)   # æ˜¾ç¤ºç¼“å­˜çŠ¶æ€
```

### Streaming Processing
```python
from waveform_analysis.core.streaming import get_streaming_context

stream_ctx = get_streaming_context(ctx, run_id="run_001", chunk_size=50000)
for chunk in stream_ctx.get_stream("st_waveforms_stream"):
    process_chunk(chunk)
```

### Adding Custom Features
```python
def my_feature_fn(self, st_waveforms, event_length, **params):
    # Returns list of feature arrays (one per channel)
    return [np.array(...) for _ in range(self.n_channels)]

ds.register_feature("my_feature", my_feature_fn, param1=value1)
ds.compute_registered_features()  # Called during build_dataframe()
```

### WaveformStruct with Custom DAQ Formats
```python
from waveform_analysis.core.processing.processor import WaveformStruct, WaveformStructConfig
from waveform_analysis.utils.formats import FormatSpec, ColumnMapping

# æ–¹å¼1: é»˜è®¤ VX2730ï¼ˆå‘åå…¼å®¹ï¼Œç°æœ‰ä»£ç æ— éœ€ä¿®æ”¹ï¼‰
struct = WaveformStruct(waveforms)
st_waveforms = struct.structure_waveforms()

# æ–¹å¼2: é€šè¿‡é€‚é…å™¨åç§°
struct = WaveformStruct.from_adapter(waveforms, "vx2730")

# æ–¹å¼3: è‡ªå®šä¹‰æ ¼å¼
custom_spec = FormatSpec(
    name="custom_daq",
    columns=ColumnMapping(
        board=0, channel=1, timestamp=3,  # æ—¶é—´æˆ³åœ¨åˆ—3
        samples_start=10,  # æ³¢å½¢æ•°æ®ä»åˆ—10å¼€å§‹
        baseline_start=10, baseline_end=50
    ),
    expected_samples=1000  # 1000ä¸ªé‡‡æ ·ç‚¹
)
config = WaveformStructConfig(format_spec=custom_spec)
struct = WaveformStruct(waveforms, config=config)

# æ–¹å¼4: åœ¨æ’ä»¶ä¸­ä½¿ç”¨
from waveform_analysis.core.context import Context
ctx = Context()
ctx.set_config({'daq_adapter': 'vx2730'}, plugin_name='st_waveforms')
st_waveforms = ctx.get_data('run_001', 'st_waveforms')

# æ–¹å¼5: åœ¨å®Œæ•´æ•°æ®æµä¸­ä½¿ç”¨ daq_adapterï¼ˆæ¨èï¼‰
from waveform_analysis.core.context import Context
from waveform_analysis.core.plugins.builtin.cpu import (
    RawFilesPlugin, WaveformsPlugin, StWaveformsPlugin
)

ctx = Context(config={"data_root": "DAQ", "n_channels": 2})
ctx.register_plugin(RawFilesPlugin())
ctx.register_plugin(WaveformsPlugin())
ctx.register_plugin(StWaveformsPlugin())

# ä¸ºæ‰€æœ‰ç›¸å…³æ’ä»¶è®¾ç½® daq_adapterï¼ˆå…¨å±€é…ç½®ï¼‰
ctx.set_config({'daq_adapter': 'vx2730'})

# æˆ–è€…ä¸ºç‰¹å®šæ’ä»¶è®¾ç½®ï¼ˆæ’ä»¶ç‰¹å®šé…ç½®ï¼‰
ctx.set_config({'daq_adapter': 'vx2730'}, plugin_name='raw_files')
ctx.set_config({'daq_adapter': 'vx2730'}, plugin_name='waveforms')
ctx.set_config({'daq_adapter': 'vx2730'}, plugin_name='st_waveforms')

# è·å–æ•°æ®ï¼ˆè‡ªåŠ¨ä½¿ç”¨é…ç½®çš„é€‚é…å™¨ï¼‰
st_waveforms = ctx.get_data('run_001', 'st_waveforms')
```

### Using New Features (Phase 2 & 3)

#### Time Range Queries
```python
from waveform_analysis.core.context import Context

ctx = Context()
# ... register plugins and set config ...

# Query specific time range (å•ä¸ªç»“æ„åŒ–æ•°ç»„)
peaks_data = ctx.time_range(
    'run_001', 'peaks',
    start_time=1000000,
    end_time=2000000
)

# Query multi-channel data (List[np.ndarray])
# è¿”å›è¿‡æ»¤åçš„åˆ—è¡¨ï¼Œæ¯ä¸ªé€šé“ä¸€ä¸ªæ•°ç»„
st_waveforms = ctx.time_range(
    'run_001', 'st_waveforms',
    start_time=1000000,
    end_time=2000000
)
print(len(st_waveforms))  # 2 (é€šé“æ•°)

# åªæŸ¥è¯¢ç‰¹å®šé€šé“
ch0_data = ctx.time_range(
    'run_001', 'st_waveforms',
    start_time=1000000,
    end_time=2000000,
    channel=0  # åªè¿”å›é€šé“0çš„æ•°æ®
)

# First query triggers index build (for better performance on repeated queries)
_ = ctx.time_range('run_001', 'st_waveforms', start_time=0, end_time=1, endtime_field='computed')

# Get index statistics
stats = ctx.get_time_index_stats()
print(f"Total indices: {stats['total_indices']}")
```

**å¤šé€šé“æ•°æ®æ”¯æŒ**ï¼š
- `time_range()` è‡ªåŠ¨æ£€æµ‹ `List[np.ndarray]` ç±»å‹ï¼Œä¸ºæ¯ä¸ªé€šé“åˆ†åˆ«å»ºç«‹ç´¢å¼•
- ç´¢å¼• key æ ¼å¼ä¸º `{data_name}_ch{i}`ï¼ˆå¦‚ `st_waveforms_ch0`ï¼‰
- `time_range()` æ”¯æŒ `channel` å‚æ•°ï¼Œå¯æŸ¥è¯¢å•ä¸ªé€šé“æˆ–æ‰€æœ‰é€šé“

#### Absolute Time Queries (ç»å¯¹æ—¶é—´æŸ¥è¯¢)

æ”¯æŒä½¿ç”¨ Python `datetime` å¯¹è±¡è¿›è¡Œæ•°æ®æŸ¥è¯¢ï¼Œéœ€è¦å…ˆè®¾ç½® epochï¼ˆæ—¶é—´åŸºå‡†ï¼‰ï¼š

```python
from datetime import datetime, timezone
from waveform_analysis.core.context import Context

ctx = Context()
# ... register plugins and set config ...

# æ–¹æ³•1: è‡ªåŠ¨ä»æ–‡ä»¶åæå– epoch
ctx.auto_extract_epoch('run_001')

# æ–¹æ³•2: æ‰‹åŠ¨è®¾ç½® epoch
epoch = datetime(2024, 1, 1, 12, 0, 0, tzinfo=timezone.utc)
ctx.set_epoch('run_001', epoch)

# ä½¿ç”¨ datetime å¯¹è±¡æŸ¥è¯¢
start = datetime(2024, 1, 1, 12, 0, 10, tzinfo=timezone.utc)
end = datetime(2024, 1, 1, 12, 0, 20, tzinfo=timezone.utc)

data = ctx.get_data_time_range_absolute(
    'run_001',
    'peaks',
    start_dt=start,
    end_dt=end
)

# æŸ¥çœ‹ epoch ä¿¡æ¯
epoch_info = ctx.get_epoch('run_001')
print(f"Epoch: {epoch_info.epoch_datetime}")
print(f"Source: {epoch_info.epoch_source}")
```

**Epoch æå–ç­–ç•¥**ï¼ˆä¼˜å…ˆçº§ä»é«˜åˆ°ä½ï¼‰ï¼š
1. `filename`: ä»æ–‡ä»¶åè§£æï¼ˆå¦‚ `data_2024-01-01_12-00-00.csv`ï¼‰
2. `csv_header`: ä» CSV å¤´éƒ¨æ³¨é‡Šæå–
3. `first_event`: ä»é¦–ä¸ªäº‹ä»¶æ—¶é—´æˆ³æ¨å¯¼

**è¯¦ç»†æ–‡æ¡£**: å‚è§ `docs/features/context/ABSOLUTE_TIME_GUIDE.md`

#### Strax Plugin Integration
```python
from waveform_analysis.core.strax_adapter import (
    wrap_strax_plugin,
    create_strax_context
)

# Wrap existing strax plugin
adapter = wrap_strax_plugin(MyStraxPlugin)
ctx.register_plugin(adapter)

# Or use strax-style API
strax_ctx = create_strax_context('./data')
strax_ctx.register(MyStraxPlugin)
data = strax_ctx.get_array('run_001', 'peaks')
df = strax_ctx.get_df('run_001', ['peaks', 'hits'])
```

#### Batch Processing
```python
from waveform_analysis.core.batch_export import BatchProcessor

processor = BatchProcessor(ctx)

# Process multiple runs in parallel
results = processor.process_runs(
    run_ids=['run_001', 'run_002', 'run_003'],
    data_name='peaks',
    max_workers=4,
    show_progress=True,
    on_error='continue'  # 'continue', 'stop', or 'raise'
)

# Access results
for run_id, data in results['results'].items():
    print(f"{run_id}: {len(data)} events")

# Check errors
if results['errors']:
    print(f"Errors: {results['errors']}")
```

#### Data Export
```python
from waveform_analysis.core.batch_export import DataExporter, batch_export

# Export single dataset
exporter = DataExporter()
exporter.export(data, 'output.parquet')  # Auto-detect format
exporter.export(data, 'output.hdf5', key='waveforms')
exporter.export(data, 'output.csv')

# Batch export multiple runs
batch_export(
    ctx,
    run_ids=['run_001', 'run_002'],
    data_name='peaks',
    output_dir='./exports',
    format='parquet',
    max_workers=4
)
```

#### Hot Reload (Development)
```python
from waveform_analysis.core.hot_reload import enable_hot_reload

# Enable auto-reload for development
reloader = enable_hot_reload(
    ctx,
    plugin_names=['my_plugin'],
    auto_reload=True,
    interval=2.0  # Check every 2 seconds
)

# Manually reload after changes
reloader.reload_plugin('my_plugin', clear_cache=True)

# Disable when done
reloader.disable_auto_reload()
```

#### PluginSpec å¥‘çº¦ç³»ç»Ÿ (é«˜çº§)

ä¸ºæ’ä»¶å®šä¹‰æœºå™¨å¯è¯»å¥‘çº¦ï¼Œç”¨äºæ³¨å†Œæ ¡éªŒã€æ–‡æ¡£ç”Ÿæˆå’Œ lineage hashï¼š

```python
from waveform_analysis.core.plugins.core import (
    Plugin, Option, PluginSpec, ConfigField,
    OutputSchema, InputRequirement, Capabilities
)

class MyPlugin(Plugin):
    provides = 'my_data'
    version = '1.0.0'
    depends_on = ['st_waveforms']
    options = {
        'threshold_mv': Option(default=10.0, type=float, help='Threshold in mV'),
    }

    # å¯é€‰ï¼šæ‰‹åŠ¨å®šä¹‰ SPEC æä¾›æ›´ä¸°å¯Œçš„å…ƒä¿¡æ¯
    SPEC = PluginSpec(
        name='MyPlugin',
        provides='my_data',
        version='1.0.0',
        depends_on=(InputRequirement(name='st_waveforms', version_spec='>=1.0.0'),),
        config_spec={
            'threshold_mv': ConfigField(
                type='float', default=10.0,
                doc='Detection threshold', units='mV'
            ),
        },
        capabilities=Capabilities(supports_streaming=False, supports_parallel=True),
    )

# è‡ªåŠ¨æå– specï¼ˆæ¨èï¼Œæ— éœ€æ‰‹åŠ¨å®šä¹‰ SPECï¼‰
spec = PluginSpec.from_plugin(MyPlugin())
print(spec.config_keys)  # ('threshold_mv',)

# ä¸¥æ ¼æ¨¡å¼æ³¨å†Œï¼ˆç”Ÿäº§ç¯å¢ƒæ¨èï¼‰
ctx.register(MyPlugin(), require_spec=True)
```

**ConfigField vs Option**:
- `Option`: è¿è¡Œæ—¶é…ç½®éªŒè¯å’Œç±»å‹è½¬æ¢ï¼ˆæ’ä»¶å¼€å‘å¿…éœ€ï¼‰
- `ConfigField`: é™æ€å¥‘çº¦å£°æ˜ï¼Œç”¨äºæ–‡æ¡£ç”Ÿæˆã€lineage hashã€IDE æç¤ºï¼ˆå¯é€‰ï¼‰

#### Lineage Visualization (è¡€ç¼˜å›¾å¯è§†åŒ–)

WaveformAnalysis æä¾›ä¸¤ç§é«˜çº§è¡€ç¼˜å›¾å¯è§†åŒ–æ¨¡å¼ï¼Œæ”¯æŒæ™ºèƒ½é¢œè‰²é«˜äº®å’Œå®Œæ•´äº¤äº’åŠŸèƒ½ã€‚

##### LabVIEW é£æ ¼ï¼ˆMatplotlibï¼‰
```python
# åŸºç¡€ç”¨æ³•
ctx.plot_lineage("df_paired", kind="labview")

# äº¤äº’å¼æ¨¡å¼ï¼ˆé¼ æ ‡æ‚¬åœæ˜¾ç¤ºè¯¦æƒ…ã€ç‚¹å‡»æ˜¾ç¤ºä¾èµ–ï¼‰
ctx.plot_lineage("df_paired", kind="labview", interactive=True)

# æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯
ctx.plot_lineage("df_paired", kind="labview", verbose=2, interactive=True)
```

##### Plotly é«˜çº§äº¤äº’å¼
```python
# Plotly æ¨¡å¼ï¼ˆå§‹ç»ˆäº¤äº’å¼ï¼Œæ”¯æŒç¼©æ”¾ã€å¹³ç§»ã€æ‚¬åœï¼‰
ctx.plot_lineage("df_paired", kind="plotly", verbose=2)

# è‡ªå®šä¹‰æ ·å¼
from waveform_analysis.core.foundation.utils import LineageStyle
style = LineageStyle(
    node_width=4.0,
    node_height=2.0,
    verbose=2
)
ctx.plot_lineage("df_paired", kind="plotly", style=style)
```

##### Verbose ç­‰çº§è¯´æ˜
- `verbose=0`: ä»…æ˜¾ç¤ºæ’ä»¶æ ‡é¢˜
- `verbose=1`: æ˜¾ç¤ºæ ‡é¢˜ + key
- `verbose=2`: æ˜¾ç¤ºæ ‡é¢˜ + key + classï¼ˆæ¨èï¼‰
- `verbose>=3`: åŒ verbose=2

##### æ™ºèƒ½é¢œè‰²é«˜äº®

ç³»ç»Ÿè‡ªåŠ¨æ ¹æ®èŠ‚ç‚¹ç±»å‹åº”ç”¨é¢œè‰²æ–¹æ¡ˆï¼š

| èŠ‚ç‚¹ç±»å‹ | é¢œè‰² | è¯†åˆ«è§„åˆ™ |
|---------|------|---------|
| åŸå§‹æ•°æ® | ğŸ”µ è“è‰²ç³» | RawFiles, Loader, Reader |
| ç»“æ„åŒ–æ•°ç»„ | ğŸŸ¢ ç»¿è‰²ç³» | å¤šå­—æ®µ dtypeï¼ˆå¦‚ `[('time', '<f8'), ...]`ï¼‰|
| DataFrame | ğŸŸ  æ©™è‰²ç³» | DataFrame, df å…³é”®è¯ |
| èšåˆæ•°æ® | ğŸŸ£ ç´«è‰²ç³» | Group, Pair, Aggregate, Merge |
| å‰¯ä½œç”¨ | ğŸŒ¸ ç²‰çº¢è‰²ç³» | Export, Save, Write |
| ä¸­é—´å¤„ç† | âšª ç°è‰²ç³» | å…¶ä»–èŠ‚ç‚¹ |

é¢œè‰²é«˜äº®åœ¨ä¸¤ç§æ¨¡å¼ä¸‹å‡è‡ªåŠ¨ç”Ÿæ•ˆï¼Œæ— éœ€é¢å¤–é…ç½®ã€‚

##### Plotly æ¨¡å¼ç‰¹æ€§

- âœ… **çœŸå®çŸ©å½¢ç»˜åˆ¶**ï¼šä½¿ç”¨ shapes API ç»˜åˆ¶èŠ‚ç‚¹å’Œç«¯å£ï¼Œå°ºå¯¸ç²¾ç¡®
- âœ… **å®Œæ•´äº¤äº’æ€§**ï¼šç¼©æ”¾ã€å¹³ç§»ã€æ‚¬åœæç¤ºã€æ¡†é€‰
- âœ… **åæ ‡åŒæ­¥ä¿®å¤**ï¼šæ‹–æ‹½æ—¶å…‰æ ‡å’ŒèŠ‚ç‚¹ä½ç½®å®Œå…¨åŒæ­¥
- âœ… **1:1 æ¯”ä¾‹ä¿æŒ**ï¼šç¡®ä¿èŠ‚ç‚¹ä¸å˜å½¢
- âœ… **ç«¯å£å¯è§**ï¼šæ˜¾ç¤ºå½©è‰²è¾“å…¥/è¾“å‡ºç«¯å£
- âœ… **ç±»å‹æ ‡æ³¨**ï¼šæ‚¬åœæç¤ºåŒ…å«èŠ‚ç‚¹ç±»å‹ä¿¡æ¯

##### æ³¨æ„äº‹é¡¹

1. **Interactive å‚æ•°**ï¼š
   - LabVIEW æ¨¡å¼ï¼š`interactive=True` å¯ç”¨ matplotlib äº¤äº’åŠŸèƒ½
   - Plotly æ¨¡å¼ï¼šå§‹ç»ˆäº¤äº’å¼ï¼Œ`interactive` å‚æ•°è¢«å¿½ç•¥

2. **æ€§èƒ½è€ƒè™‘**ï¼š
   - LabVIEW æ¨¡å¼é€‚åˆé™æ€å¯¼å‡ºå’Œç®€å•äº¤äº’
   - Plotly æ¨¡å¼é€‚åˆå¤æ‚å›¾å½¢çš„æ·±åº¦æ¢ç´¢

3. **ä¾èµ–**ï¼š
   - LabVIEW æ¨¡å¼ï¼šéœ€è¦ matplotlibï¼ˆæ ‡å‡†ä¾èµ–ï¼‰
   - Plotly æ¨¡å¼ï¼šéœ€è¦ `pip install plotly`

## Cache Management (ç¼“å­˜ç®¡ç†)

WaveformAnalysis æä¾›å®Œæ•´çš„ç¼“å­˜ç®¡ç†å·¥å…·é›†ï¼Œç”¨äºåˆ†æã€è¯Šæ–­ã€æ¸…ç†å’Œç»Ÿè®¡ç¼“å­˜æ•°æ®ã€‚

### Python API

#### å¿«é€Ÿä½¿ç”¨

```python
# è·å–ç¼“å­˜åˆ†æå™¨
analyzer = ctx.analyze_cache()

# æŸ¥çœ‹ç¼“å­˜ç»Ÿè®¡
stats = ctx.cache_stats(detailed=True)

# è¯Šæ–­ç¼“å­˜é—®é¢˜
issues = ctx.diagnose_cache(run_id='run_001')

# è‡ªåŠ¨ä¿®å¤ï¼ˆdry-runï¼‰
ctx.diagnose_cache(run_id='run_001', auto_fix=True, dry_run=True)
```

#### é«˜çº§ç”¨æ³•

```python
from waveform_analysis.core.storage import (
    CacheAnalyzer,
    CacheDiagnostics,
    CacheCleaner,
    CacheStatsCollector,
    CleanupStrategy,
)

# 1. åˆ†æç¼“å­˜
analyzer = CacheAnalyzer(ctx)
analyzer.scan()

# è·å–æ‰€æœ‰æ¡ç›®
entries = analyzer.get_entries()

# æŒ‰æ¡ä»¶è¿‡æ»¤
large = analyzer.get_entries(min_size=1024*1024)  # > 1MB
old = analyzer.get_entries(max_age_days=30)       # > 30 å¤©
run_entries = analyzer.get_entries(run_id='run_001')

# æ‰“å°æ‘˜è¦
analyzer.print_summary(detailed=True)

# 2. è¯Šæ–­é—®é¢˜
diag = CacheDiagnostics(analyzer)
issues = diag.diagnose()
diag.print_report(issues)

# è‡ªåŠ¨ä¿®å¤
result = diag.auto_fix(issues, dry_run=True)

# 3. æ™ºèƒ½æ¸…ç†
cleaner = CacheCleaner(analyzer)

# åˆ›å»ºæ¸…ç†è®¡åˆ’
plan = cleaner.plan_cleanup(
    strategy=CleanupStrategy.LRU,
    target_size_mb=1024
)
cleaner.preview_plan(plan, detailed=True)

# æ‰§è¡Œæ¸…ç†
cleaner.execute(plan, dry_run=False)

# æŒ‰å¹´é¾„æ¸…ç†
cleaner.cleanup_by_age(max_age_days=30, dry_run=True)

# æ¸…ç†åˆ°ç›®æ ‡å¤§å°
cleaner.cleanup_to_target_size(target_total_mb=500, dry_run=True)

# 4. ç»Ÿè®¡æ”¶é›†
collector = CacheStatsCollector(analyzer)
stats = collector.collect()
collector.print_summary(stats, detailed=True)

# å¯¼å‡ºç»Ÿè®¡
collector.export_stats(stats, 'cache_stats.json')
```

### æ¸…ç†ç­–ç•¥

| ç­–ç•¥ | è¯´æ˜ |
|------|------|
| `LRU` | æŒ‰åˆ›å»ºæ—¶é—´æ’åºï¼Œåˆ é™¤æœ€æ—§çš„ |
| `OLDEST` | æœ€æ—§çš„ä¼˜å…ˆ |
| `LARGEST` | æœ€å¤§çš„ä¼˜å…ˆ |
| `VERSION_MISMATCH` | æ’ä»¶ç‰ˆæœ¬ä¸åŒ¹é…çš„ |
| `FAILED_INTEGRITY` | å®Œæ•´æ€§æ£€æŸ¥å¤±è´¥çš„ |
| `BY_RUN` | æŒ‰è¿è¡Œæ¸…ç† |
| `BY_DATA_TYPE` | æŒ‰æ•°æ®ç±»å‹æ¸…ç† |

### CLI å‘½ä»¤

```bash
# ç¼“å­˜æ¦‚è§ˆ
waveform-cache info [--run RUN_ID] [--detailed] [--storage-dir PATH]

# è¯¦ç»†ç»Ÿè®¡
waveform-cache stats [--run RUN_ID] [--detailed] [--export stats.json]

# è¯Šæ–­é—®é¢˜
waveform-cache diagnose [--run RUN_ID] [--fix] [--dry-run]

# åˆ—å‡ºç¼“å­˜æ¡ç›®
waveform-cache list [--run RUN_ID] [--data-type TYPE] [--min-size BYTES]

# æ¸…ç†ç¼“å­˜
waveform-cache clean --strategy lru --size-mb 500 [--dry-run]
waveform-cache clean --strategy oldest --days 30 [--no-dry-run]
waveform-cache clean --strategy largest --max-entries 10 --dry-run
```

### è¯Šæ–­é—®é¢˜ç±»å‹

| ç±»å‹ | ä¸¥é‡æ€§ | è¯´æ˜ |
|------|--------|------|
| `VERSION_MISMATCH` | warning | æ’ä»¶ç‰ˆæœ¬ä¸ç¼“å­˜ä¸åŒ¹é… |
| `MISSING_METADATA` | error | å…ƒæ•°æ®æ–‡ä»¶ç¼ºå¤± |
| `MISSING_DATA_FILE` | error | æ•°æ®æ–‡ä»¶ç¼ºå¤± |
| `SIZE_MISMATCH` | error | æ–‡ä»¶å¤§å°ä¸åŒ¹é… |
| `CHECKSUM_FAILED` | error | æ ¡éªŒå’ŒéªŒè¯å¤±è´¥ |
| `ORPHAN_FILE` | warning | å­¤å„¿æ–‡ä»¶ï¼ˆæ— å…ƒæ•°æ®ï¼‰ |
| `STORAGE_VERSION_MISMATCH` | warning | å­˜å‚¨ç‰ˆæœ¬ä¸åŒ¹é… |

### å®‰å…¨ç‰¹æ€§

- **é»˜è®¤ dry-run**: æ‰€æœ‰æ¸…ç†å’Œä¿®å¤æ“ä½œé»˜è®¤ä¸ºæ¼”ç»ƒæ¨¡å¼
- **çº¿ç¨‹å®‰å…¨**: CacheAnalyzer ä½¿ç”¨é”ä¿æŠ¤ç¼“å­˜ç´¢å¼•
- **å¢é‡æ‰«æ**: æ”¯æŒå¢é‡æ‰«æé¿å…é‡å¤éå†
- **è¯¦ç»†é¢„è§ˆ**: æ‰§è¡Œå‰å¯é¢„è§ˆæ‰€æœ‰å°†è¦æ‰§è¡Œçš„æ“ä½œ

## Common Pitfalls

1. **Generator Exhaustion**: Generators can only be consumed once; repeat access triggers recomputation
2. **Missing run_id**: Always pass `run_id` to `Context.get_data()` to avoid data conflicts
3. **Cache Invalidation**: Bump `version` when changing plugin logic/dtype/options
4. **Data Paths**: Default data directory is `DAQ/<run_name>`; missing files cause `FileNotFoundError`
5. **Chunk Boundaries**: Record endtime must not exceed chunk boundary; validate with `check_chunk_boundaries()`
6. **Timestamp Index**: After modifying `st_waveforms`, call `_build_timestamp_index()` to rebuild index
7. **Waveform Access**: With `load_waveforms=False`, `get_waveform_at()` returns None
8. **Plugin dtype**: `output_dtype` can be either:
   - Valid NumPy dtype (e.g., `np.dtype([('time', '<f8'), ('charge', '<f4')])`)
   - Type annotation strings for non-array outputs (e.g., `"List[np.ndarray]"`, `"pd.DataFrame"`)
   - Framework automatically handles both cases in lineage tracking and validation

## Testing Notes

- Test script auto-activates conda environment `pyroot-kernel`
- If DAQ data files missing, tests will `pytest.skip()` gracefully
- Coverage report generated in `htmlcov/` directory
- Use `scripts/benchmark_io.py` to test I/O performance with different chunksizes

## File Structure Notes

- `waveform_analysis/core/`: Core processing logic (modular subdirectories since 2026-01)
  - `context.py`, `dataset.py`: Core files (root level)
  - `config/`: Configuration system (resolver, compat, adapter_info, types) [NEW - 2026-01]
    - `resolver.py`: ConfigResolver é…ç½®è§£æå™¨
    - `compat.py`: CompatManager å…¼å®¹å±‚ç®¡ç†
    - `adapter_info.py`: AdapterInfo DAQ adapter ä¿¡æ¯
    - `types.py`: ConfigSource, ConfigValue, ResolvedConfig ç±»å‹å®šä¹‰
  - `storage/`: Storage layer (memmap, backends, cache, compression, integrity)
    - `cache_analyzer.py`: ç¼“å­˜åˆ†æå™¨å’Œ CacheEntry æ•°æ®ç±»
    - `cache_diagnostics.py`: ç¼“å­˜è¯Šæ–­å’Œä¿®å¤å·¥å…·
    - `cache_cleaner.py`: æ™ºèƒ½ç¼“å­˜æ¸…ç†ç­–ç•¥
    - `cache_statistics.py`: ç¼“å­˜ç»Ÿè®¡æ”¶é›†å’ŒæŠ¥å‘Š
  - `execution/`: Execution layer (manager, config, timeout)
  - `plugins/`: Plugin system (æŒ‰åŠ é€Ÿå™¨åˆ’åˆ†æ¶æ„ï¼Œsince 2026-01)
    - `core/`: Plugin infrastructure (base, streaming, adapters, hot_reload, spec)
      - `spec.py`: PluginSpec, ConfigField, OutputSchema, Capabilities [NEW - 2026-01]
    - `builtin/`: Built-in plugins organized by accelerator
      - `cpu/`: CPU implementations (NumPy/SciPy/Numba)
        - `standard.py`: 10 standard data processing plugins
        - `filtering.py`: FilteredWaveformsPlugin (Butterworth, Savitzky-Golay)
        - `peak_finding.py`: SignalPeaksPlugin (scipy.signal.find_peaks)
      - `jax/`: JAX GPU implementations (å¾…å¼€å‘ - Phase 2)
      - `streaming/`: Streaming plugins (CPU å·²æœ‰ SignalPeaksStreamPlugin)
        - `cpu/`: CPU streaming plugins (SignalPeaksStreamPlugin)
        - `jax/`: JAX streaming plugins (å¾…å¼€å‘)
      - `legacy/`: Deprecated plugins for backward compatibility
        - `__init__.py`: Lazy import with deprecation warnings
        - `standard.py`: Original standard plugins
        - `signal_processing.py`: Original signal processing plugins
  - `processing/`: Data processing (loader, processor, analyzer, chunk)
  - `data/`: Data management (query, export)
  - `foundation/`: Framework basics (exceptions, mixins, model, utils, progress)
- `waveform_analysis/cli_cache.py`: ç¼“å­˜ç®¡ç† CLI å‘½ä»¤ (waveform-cache)
- `waveform_analysis/utils/`: Utilities (DAQ adapters, I/O, visualization)
- `waveform_analysis/fitting/`: Physics fitting models
- `tests/`: Unit and integration tests
  - `test_cache_analyzer.py`: CacheAnalyzer æµ‹è¯•
  - `test_cache_diagnostics.py`: CacheDiagnostics æµ‹è¯•
  - `test_cache_cleaner.py`: CacheCleaner æµ‹è¯•
  - `test_cache_statistics.py`: CacheStatsCollector æµ‹è¯•
- `examples/`: Usage demonstrations
- `docs/`: Architecture, guides, and implementation details
- `scripts/`: Helper scripts (testing, benchmarking)
- `DAQ/`: Data directory (not in package)
- `outputs/`: Results directory (not in package)

## Key Documentation Files

- `docs/architecture/ARCHITECTURE.md`: Complete architecture and data flow
- `docs/features/context/DATA_ACCESS.md`: Data access, cache behavior, and cache analysis
- `docs/features/context/CONFIGURATION.md`: Configuration management and resolved config
- `docs/plugins/guides/STREAMING_PLUGINS_GUIDE.md`: Streaming framework usage
- `docs/features/advanced/EXECUTOR_MANAGER_GUIDE.md`: Parallel execution management
- `docs/user-guide/QUICKSTART_GUIDE.md`: Quick start examples
- `docs/features/context/PREVIEW_EXECUTION.md`: Preview execution plans before running
- `docs/plugins/tutorials/SIGNAL_PROCESSING_PLUGINS.md`: Signal processing plugins (filtering, peak detection)
- `docs/development/plugin-development/PLUGIN_SPEC_GUIDE.md`: PluginSpec and ConfigField advanced guide [NEW]
- **Lineage Visualization**: See `CLAUDE.md` Â§ Lineage Visualization for color-coded interactive graph features
- `.github/copilot-instructions.md`: Detailed development guidelines (Chinese)
