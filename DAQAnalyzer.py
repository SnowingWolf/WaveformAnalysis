import json
import os
import re
from datetime import datetime
from pathlib import Path

import numpy as np
import pandas as pd

try:
    from IPython.display import display
except ImportError:

    def display(x):
        print(x)


class DAQRun:
    """å•ä¸ª DAQ è¿è¡Œçš„æ•°æ®å’Œåˆ†æç±»"""

    # ä»…å¤„ç† CSV æ•°æ®æ–‡ä»¶ï¼Œé¿å…è¯¯è¯» .root/.dat
    ALLOWED_EXTS = (".CSV", ".csv")
    CH_PATTERN = re.compile(r"CH(\d+)")
    IDX_PATTERN = re.compile(r"_(\d+)\.CSV$", re.IGNORECASE)

    def __init__(self, run_name, run_path):
        self.run_name = run_name
        self.run_path = run_path
        self.raw_dir = os.path.join(run_path, "RAW")

        # åŸºæœ¬ä¿¡æ¯
        self.description = self._load_description()
        self.total_bytes = 0
        self.file_count = 0
        self.channels = set()

        # æ—¶é—´æˆ³ä¿¡æ¯ (ps ä¸ºå•ä½)
        self.channel_files = {}  # {channel: [list of file info]}
        self.channel_stats = {}  # {channel: {stats}}
        self.ps_per_ns = 1000  # 1 ns = 1000 ps
        self.ps_per_us = 1e6  # 1 us = 1e6 ps
        self.ps_per_s = 1e12  # 1 s = 1e12 ps

        self._scan_channel_files()
        self._compute_channel_stats()

    def _load_description(self):
        """è¯»å–æè¿°ä¿¡æ¯"""
        info_file = os.path.join(self.run_path, f"{self.run_name}_info.txt")
        if os.path.exists(info_file):
            with open(info_file, "r", encoding="utf-8") as f:
                return f.readline().strip()
        return "æ— æè¿°"

    def _scan_channel_files(self):
        """æ‰«æ RAW ç›®å½•ä¸‹çš„æ‰€æœ‰æ–‡ä»¶ï¼ŒæŒ‰é€šé“ç»„ç»‡"""
        if not os.path.isdir(self.raw_dir):
            return

        for fname in sorted(os.listdir(self.raw_dir)):
            if not fname.endswith(self.ALLOWED_EXTS):
                continue

            fpath = os.path.join(self.raw_dir, fname)
            size_bytes = os.path.getsize(fpath)
            mtime = datetime.fromtimestamp(os.path.getmtime(fpath))

            # æå–é€šé“å’Œç´¢å¼•
            ch_match = self.CH_PATTERN.search(fname)
            ch = int(ch_match.group(1)) if ch_match else None
            idx_match = self.IDX_PATTERN.search(fname)
            idx = int(idx_match.group(1)) if idx_match else 0

            if ch is not None:
                if ch not in self.channel_files:
                    self.channel_files[ch] = []

                self.channel_files[ch].append({
                    "filename": fname,
                    "index": idx,
                    "path": fpath,
                    "size_bytes": size_bytes,
                    "mtime": mtime,
                    "timetag_min": None,
                    "timetag_max": None,
                    "timetag_min_ns": None,
                    "timetag_max_ns": None,
                })

                self.channels.add(ch)
                self.total_bytes += size_bytes
                self.file_count += 1

    def _compute_channel_stats(self):
        """åˆå§‹åŒ–ç»Ÿè®¡æ•°æ®ï¼ˆå»¶è¿Ÿè®¡ç®—ï¼‰"""
        pass

    def _parse_csv_file(self, fpath):
        """è§£æ CSV æ–‡ä»¶çš„æ—¶é—´æˆ³åˆ—ï¼ˆç¬¬3åˆ— TIMETAGï¼‰
        åªè¯»å–ç¬¬ä¸€è¡Œå’Œæœ€åä¸€è¡Œä»¥è·å–å¼€å§‹å’Œç»“æŸæ—¶é—´æˆ³
        è¿”å› (min_timetag_ps, max_timetag_ps)
        """
        try:
            start_tag = None
            end_tag = None

            with open(fpath, "r", encoding="utf-8", errors="ignore") as f:
                # è¯»å–ç¬¬ä¸€è¡Œï¼ˆheaderï¼‰
                header = f.readline()

                # è¯»å–ç¬¬äºŒè¡Œï¼ˆç¬¬ä¸€æ¡æ•°æ®ï¼‰
                first_line = f.readline().strip()
                if first_line:
                    first_parts = first_line.split(";")
                    if len(first_parts) >= 3:
                        start_tag = int(first_parts[2])

                # è¯»å–æ–‡ä»¶çš„æœ€åä¸€è¡Œ
                # ä»æœ«å°¾å¼€å§‹ï¼Œæ¯æ¬¡å¾€å‰è¯» 4KB å—
                f.seek(0, 2)  # ç§»åˆ°æ–‡ä»¶æœ«å°¾
                file_size = f.tell()

                # è®¾ç½®ç¼“å†²åŒºå¤§å°ï¼ˆé¿å…ä¸€æ¬¡æ€§è¯»å–å¤§æ–‡ä»¶ï¼‰
                buffer_size = min(4096, file_size)
                pos = max(0, file_size - buffer_size)
                f.seek(pos)

                # è¯»å–æœ«å°¾çš„å—ï¼Œæ‰¾åˆ°æœ€åä¸€è¡Œ
                chunk = f.read()
                lines = chunk.split("\n")

                # ä»åå¾€å‰æ‰¾éç©ºè¡Œ
                for i in range(len(lines) - 1, -1, -1):
                    last_line = lines[i].strip()
                    if last_line:
                        last_parts = last_line.split(";")
                        if len(last_parts) >= 3:
                            end_tag = int(last_parts[2])
                        break

            if start_tag is not None and end_tag is not None:
                return start_tag, end_tag

        except Exception as e:
            # é™é»˜å¤„ç†ï¼Œä¸æ‰“å°è­¦å‘Šä»¥æé«˜æ€§èƒ½
            pass

        return None, None

    def compute_acquisition_times(self, force_reparse=False):
        """è®¡ç®—æ¯ä¸ªé€šé“çš„é‡‡é›†æ—¶é—´ï¼ˆå¼€å§‹å’Œç»“æŸï¼‰

        è¿”å›ï¼š
            dict: {channel: {
                'file_count': int,
                'total_size_bytes': int,
                'start_time_ps': int,
                'end_time_ps': int,
                'start_time_us': float,
                'end_time_us': float,
                'duration_s': float,
                'earliest_mtime': datetime,
                'latest_mtime': datetime,
            }}
        """
        if self.channel_stats and not force_reparse:
            return self.channel_stats

        for ch in sorted(self.channel_files.keys()):
            files = self.channel_files[ch]
            files_sorted = sorted(files, key=lambda x: x["index"])

            min_tag_ps = None
            max_tag_ps = None
            earliest_mtime = None
            latest_mtime = None

            # éå†æ¯ä¸ªæ–‡ä»¶ï¼Œè·å–æ—¶é—´æˆ³èŒƒå›´
            for file_info in files_sorted:
                min_t, max_t = self._parse_csv_file(file_info["path"])
                if min_t is not None:
                    file_info["timetag_min"] = min_t
                    file_info["timetag_max"] = max_t
                    file_info["timetag_min_ns"] = min_t // self.ps_per_ns
                    file_info["timetag_max_ns"] = max_t // self.ps_per_ns

                    if min_tag_ps is None or min_t < min_tag_ps:
                        min_tag_ps = min_t
                    if max_tag_ps is None or max_t > max_tag_ps:
                        max_tag_ps = max_t

                # è®°å½•æ–‡ä»¶ä¿®æ”¹æ—¶é—´èŒƒå›´
                mtime = file_info["mtime"]
                if earliest_mtime is None or mtime < earliest_mtime:
                    earliest_mtime = mtime
                if latest_mtime is None or mtime > latest_mtime:
                    latest_mtime = mtime

            # è½¬æ¢ä¸ºå…¶ä»–å•ä½
            start_us = min_tag_ps / self.ps_per_us if min_tag_ps is not None else None
            end_us = max_tag_ps / self.ps_per_us if max_tag_ps is not None else None

            if min_tag_ps is not None and max_tag_ps is not None:
                duration_s = (max_tag_ps - min_tag_ps) / self.ps_per_s
            else:
                duration_s = None

            self.channel_stats[ch] = {
                "file_count": len(files),
                "total_size_bytes": sum(f["size_bytes"] for f in files),
                "start_time_ps": min_tag_ps,
                "end_time_ps": max_tag_ps,
                "start_time_us": start_us,
                "end_time_us": end_us,
                "duration_s": duration_s,
                "earliest_mtime": earliest_mtime,
                "latest_mtime": latest_mtime,
            }

        return self.channel_stats

    def get_channel_summary(self):
        """è·å–æ‰€æœ‰é€šé“çš„æ±‡æ€»ç»Ÿè®¡"""
        if not self.channel_stats:
            self.compute_acquisition_times()
        return self.channel_stats

    def get_channel_file_details(self, channel):
        """è·å–æŒ‡å®šé€šé“çš„æ‰€æœ‰æ–‡ä»¶è¯¦æƒ…"""
        if channel not in self.channel_files:
            return None
        return sorted(self.channel_files[channel], key=lambda x: x["index"])

    def to_dict(self):
        """è½¬æ¢ä¸ºå­—å…¸æ ¼å¼ï¼ˆç”¨äºåˆ›å»º DataFrameï¼‰"""
        return {
            "run_name": self.run_name,
            "description": self.description,
            "file_count": self.file_count,
            "total_size_mb": self.total_bytes / (1024**2) if self.total_bytes > 0 else 0,
            "total_bytes": self.total_bytes,
            "channel_count": len(self.channels),
            "channels": sorted(list(self.channels)),
            "channel_str": ", ".join(map(str, sorted(list(self.channels)))) if self.channels else "-",
            "path": self.run_path,
        }

    @staticmethod
    def format_time_ps(ps_val):
        """æ ¼å¼åŒ– ps ä¸ºæ˜“è¯»çš„æ—¶é—´"""
        if ps_val is None:
            return "N/A"

        if ps_val < 1e3:
            return f"{ps_val:.0f} ps"
        elif ps_val < 1e6:
            return f"{ps_val / 1e3:.2f} ns"
        elif ps_val < 1e9:
            return f"{ps_val / 1e6:.2f} us"
        elif ps_val < 1e12:
            return f"{ps_val / 1e9:.2f} ms"
        else:
            return f"{ps_val / 1e12:.2f} s"


class DAQAnalyzer:
    """DAQ æ•°æ®åˆ†æå™¨ï¼šç®¡ç†æ‰€æœ‰è¿è¡Œçš„ç»Ÿä¸€åˆ†æ"""

    def __init__(self, daq_root="DAQ"):
        self.daq_root = daq_root
        self.runs = {}  # {run_name: DAQRun}
        self.df_runs = None
        self.total_bytes = 0

    @staticmethod
    def format_size(bytes_val):
        """å°†å­—èŠ‚è½¬æ¢ä¸ºæ˜“è¯»æ ¼å¼"""
        for unit in ["B", "KB", "MB", "GB", "TB"]:
            if bytes_val < 1024:
                return f"{bytes_val:.2f} {unit}"
            bytes_val /= 1024
        return f"{bytes_val:.2f} PB"

    def scan_all_runs(self):
        """æ‰«ææ‰€æœ‰è¿è¡Œ"""
        if not os.path.exists(self.daq_root):
            print(f"é”™è¯¯: æ‰¾ä¸åˆ°ç›®å½• {self.daq_root}")
            return self

        self.runs = {}
        self.total_bytes = 0

        for run_name in sorted(os.listdir(self.daq_root)):
            run_path = os.path.join(self.daq_root, run_name)

            if not os.path.isdir(run_path):
                continue

            # åˆ›å»º DAQRun å¯¹è±¡
            run = DAQRun(run_name, run_path)
            self.runs[run_name] = run
            self.total_bytes += run.total_bytes

        # ç”Ÿæˆ DataFrame
        self._build_dataframe()
        return self

    def _build_dataframe(self):
        """ä»æ‰€æœ‰è¿è¡Œç”Ÿæˆ DataFrame"""
        run_dicts = [run.to_dict() for run in self.runs.values()]
        self.df_runs = pd.DataFrame(run_dicts)

    def get_run(self, run_name):
        """è·å–æŒ‡å®šè¿è¡Œçš„ DAQRun å¯¹è±¡"""
        return self.runs.get(run_name)

    def get_all_runs(self):
        """è·å–æ‰€æœ‰è¿è¡Œ"""
        return list(self.runs.values())

    def display_overview(self):
        """æ˜¾ç¤ºæ‰€æœ‰è¿è¡Œçš„æ¦‚è§ˆè¡¨"""
        if self.df_runs is None or self.df_runs.empty:
            print("æœªå‘ç°ä»»ä½•æœ‰æ•ˆçš„è¿è¡Œæ•°æ®ã€‚")
            return self

        print(f"ğŸ“Š æ•°æ®æ±‡æ€»: å…±æ‰¾åˆ° {len(self.df_runs)} ä¸ªè¿è¡Œé¡¹ç›® | æ€»å ç”¨ç©ºé—´: {self.format_size(self.total_bytes)}")
        print("\n")

        styled_df = (
            self.df_runs[["run_name", "description", "file_count", "total_size_mb", "channel_count", "channel_str"]]
            .rename(
                columns={
                    "run_name": "è¿è¡Œåç§°",
                    "description": "æè¿°",
                    "file_count": "æ–‡ä»¶æ•°",
                    "total_size_mb": "å¤§å°(MB)",
                    "channel_count": "é€šé“æ•°",
                    "channel_str": "é€šé“åˆ—è¡¨",
                }
            )
            .style.background_gradient(subset=["æ–‡ä»¶æ•°", "é€šé“æ•°"], cmap="Blues")
            .background_gradient(subset=["å¤§å°(MB)"], cmap="Reds")
            .format({"å¤§å°(MB)": "{:.2f}"})
            .set_properties(**{"text-align": "left"})
            .set_table_styles([
                {
                    "selector": "th",
                    "props": [
                        ("background-color", "#4CAF50"),
                        ("color", "white"),
                        ("font-weight", "bold"),
                        ("text-align", "center"),
                    ],
                }
            ])
            .hide(axis="index")
        )
        display(styled_df)
        return self

    def display_summary(self):
        """æ˜¾ç¤ºè¯¦ç»†ç»Ÿè®¡ä¿¡æ¯"""
        if self.df_runs is None or self.df_runs.empty:
            print("æœªå‘ç°ä»»ä½•æœ‰æ•ˆçš„è¿è¡Œæ•°æ®ã€‚")
            return self

        print("=" * 80)
        print("è¯¦ç»†ç»Ÿè®¡ä¿¡æ¯")
        print("=" * 80)

        print(f"\næ€»è¿è¡Œé¡¹ç›®æ•°: {len(self.df_runs)}")
        print(f"æ€»æ–‡ä»¶æ•°: {self.df_runs['file_count'].sum()}")
        print(f"æ€»å ç”¨ç©ºé—´: {self.format_size(self.df_runs['total_bytes'].sum())}")
        print(f"å¹³å‡æ–‡ä»¶æ•°/é¡¹ç›®: {self.df_runs['file_count'].mean():.1f}")
        print(f"å¹³å‡ç©ºé—´/é¡¹ç›®: {self.format_size(self.df_runs['total_bytes'].mean())}")

        if self.df_runs["file_count"].max() > 0:
            print(
                f"\næœ€å¤§æ–‡ä»¶æ•°é¡¹ç›®: {self.df_runs.loc[self.df_runs['file_count'].idxmax(), 'run_name']} ({self.df_runs['file_count'].max()} ä¸ªæ–‡ä»¶)"
            )
            print(
                f"æœ€å¤§ç©ºé—´é¡¹ç›®: {self.df_runs.loc[self.df_runs['total_bytes'].idxmax(), 'run_name']} ({self.format_size(self.df_runs['total_bytes'].max())})"
            )

        # é€šé“ä½¿ç”¨æƒ…å†µ
        all_channels = set()
        for ch_list in self.df_runs["channel_str"]:
            if ch_list != "-":
                channels = [int(x.strip()) for x in ch_list.split(",")]
                all_channels.update(channels)

        if all_channels:
            print(f"\nä½¿ç”¨çš„é€šé“: {sorted(all_channels)}")
            print(f"é€šé“æ€»æ•°: {len(all_channels)}")

        # ç©ºé¡¹ç›®æ£€æŸ¥
        empty_runs = self.df_runs[self.df_runs["file_count"] == 0]
        if not empty_runs.empty:
            print(f"\nâš ï¸  æ— æ•°æ®çš„é¡¹ç›® ({len(empty_runs)} ä¸ª):")
            for name in empty_runs["run_name"]:
                print(f"  - {name}")

        print("=" * 80)
        return self

    def display_run_channel_details(self, run_name, show_files: bool = False):
        """æ˜¾ç¤ºæŒ‡å®šè¿è¡Œçš„é€šé“é‡‡é›†æ—¶é—´å’Œæ–‡ä»¶è¯¦æƒ…ï¼ˆè¡¨æ ¼å½¢å¼ï¼‰ã€‚

        Args:
            run_name (str): è¿è¡Œå
            show_files (bool): æ˜¯å¦æ˜¾ç¤ºæ¯ä¸ªé€šé“çš„æ–‡ä»¶æ˜ç»†è¡¨ï¼ˆé»˜è®¤ Falseï¼‰
        """
        run = self.get_run(run_name)
        if run is None:
            print(f"é”™è¯¯: æ‰¾ä¸åˆ°è¿è¡Œ {run_name}")
            return self

        # è®¡ç®—é‡‡é›†æ—¶é—´
        run.compute_acquisition_times()
        stats = run.get_channel_summary()

        print(f"\n{'=' * 80}")
        print(f"è¿è¡Œ: {run_name}")
        print(f"{'=' * 80}")

        # æ±‡æ€»è¡¨æ ¼
        rows = []
        for ch in sorted(stats.keys()):
            s = stats[ch]
            start_readable = DAQRun.format_time_ps(s.get("start_time_ps"))
            end_readable = DAQRun.format_time_ps(s.get("end_time_ps"))
            duration = f"{s['duration_s']:.3f} s" if s.get("duration_s") is not None else "N/A"

            rows.append({
                "channel": f"CH{ch}",
                "files": s.get("file_count"),
                "total_size": self.format_size(s.get("total_size_bytes", 0)),
                "start_time": start_readable,
                "end_time": end_readable,
                "duration": duration,
                "earliest_file": s.get("earliest_mtime"),
                "latest_file": s.get("latest_mtime"),
            })

        try:
            df = pd.DataFrame(rows).set_index("channel")
            display(df)
        except Exception:
            # é€€å›åˆ°æ‰“å°æ¨¡å¼ï¼ˆæä½æ¦‚ç‡è§¦å‘ï¼‰
            print("\nã€é€šé“é‡‡é›†æ—¶é—´ç»Ÿè®¡ã€‘")
            for r in rows:
                print(
                    f"  {r['channel']}: files={r['files']}, size={r['total_size']}, start={r['start_time']}, end={r['end_time']}, duration={r['duration']}"
                )

        # å¯é€‰ï¼šæ˜¾ç¤ºæ¯ä¸ªé€šé“çš„æ–‡ä»¶æ˜ç»†
        if show_files:
            for ch in sorted(stats.keys()):
                files = run.get_channel_file_details(ch)
                if not files:
                    continue

                print(f"\n-- æ–‡ä»¶æ˜ç»† CH{ch} --")
                frows = []
                for fi in sorted(files, key=lambda x: x.get("index", 0)):
                    frows.append({
                        "filename": fi.get("filename"),
                        "index": fi.get("index"),
                        "size": self.format_size(fi.get("size_bytes", 0)),
                        "timetag_min": DAQRun.format_time_ps(fi.get("timetag_min")),
                        "timetag_max": DAQRun.format_time_ps(fi.get("timetag_max")),
                        "modified": fi.get("mtime"),
                    })

                try:
                    fdf = pd.DataFrame(frows).set_index("index")
                    display(fdf)
                except Exception:
                    for fr in frows:
                        print(fr)

        return self

    def save_to_json(self, output_path="daq_analysis.json", include_file_details=True):
        """ä¿å­˜æ‰«æç»“æœåˆ° JSON æ–‡ä»¶

        Args:
            output_path (str): è¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼Œé»˜è®¤ä¸º 'daq_analysis.json'
            include_file_details (bool): æ˜¯å¦åŒ…å«æ¯ä¸ªæ–‡ä»¶çš„è¯¦ç»†ä¿¡æ¯

        Returns:
            str: è¾“å‡ºæ–‡ä»¶è·¯å¾„
        """
        if self.df_runs is None or self.df_runs.empty:
            print("é”™è¯¯: å°šæœªæ‰«æè¿è¡Œæ•°æ®ï¼Œè¯·å…ˆè°ƒç”¨ scan_all_runs()")
            return None

        # ç¡®ä¿æ‰€æœ‰è¿è¡Œéƒ½å·²è®¡ç®—é‡‡é›†æ—¶é—´
        for run in self.runs.values():
            run.compute_acquisition_times()

        # æ„å»ºè¾“å‡ºæ•°æ®
        output_data = {
            "metadata": {
                "scan_time": datetime.now().isoformat(),
                "daq_root": self.daq_root,
                "total_runs": len(self.runs),
                "total_files": sum(run.file_count for run in self.runs.values()),
                "total_size_bytes": self.total_bytes,
                "total_size_readable": self.format_size(self.total_bytes),
            },
            "runs": [],
        }

        # æ·»åŠ æ¯ä¸ªè¿è¡Œçš„æ•°æ®
        for run_name in sorted(self.runs.keys()):
            run = self.runs[run_name]
            stats = run.get_channel_summary()

            # åŸºæœ¬è¿è¡Œä¿¡æ¯
            run_data = {
                "run_name": run.run_name,
                "description": run.description,
                "file_count": run.file_count,
                "total_size_bytes": run.total_bytes,
                "total_size_readable": self.format_size(run.total_bytes),
                "path": run.run_path,
                "channels": sorted(run.channels),
                "channel_details": {},
            }

            # æ·»åŠ æ¯ä¸ªé€šé“çš„è¯¦æƒ…
            for ch in sorted(stats.keys()):
                s = stats[ch]
                channel_data = {
                    "channel": ch,
                    "file_count": s["file_count"],
                    "total_size_bytes": s["total_size_bytes"],
                    "total_size_readable": self.format_size(s["total_size_bytes"]),
                    "start_time_ps": s["start_time_ps"],
                    "end_time_ps": s["end_time_ps"],
                    "duration_seconds": s["duration_s"],
                    "earliest_file_time": s["earliest_mtime"].isoformat() if s["earliest_mtime"] else None,
                    "latest_file_time": s["latest_mtime"].isoformat() if s["latest_mtime"] else None,
                }

                # æ·»åŠ æ–‡ä»¶è¯¦æƒ…ï¼ˆå¯é€‰ï¼‰
                if include_file_details:
                    files = run.get_channel_file_details(ch)
                    channel_data["files"] = []
                    if files:
                        for file_info in files:
                            file_data = {
                                "filename": file_info["filename"],
                                "index": file_info["index"],
                                "size_bytes": file_info["size_bytes"],
                                "size_readable": self.format_size(file_info["size_bytes"]),
                                "modified_time": file_info["mtime"].isoformat(),
                                "timetag_min_ps": file_info["timetag_min"],
                                "timetag_max_ps": file_info["timetag_max"],
                                "timetag_min_readable": DAQRun.format_time_ps(file_info["timetag_min"]),
                                "timetag_max_readable": DAQRun.format_time_ps(file_info["timetag_max"]),
                            }
                            channel_data["files"].append(file_data)

                run_data["channel_details"][str(ch)] = channel_data

            output_data["runs"].append(run_data)

        # ä¿å­˜åˆ°æ–‡ä»¶
        try:
            with open(output_path, "w", encoding="utf-8") as f:
                json.dump(output_data, f, indent=2, ensure_ascii=False)

            print(f"âœ“ æ•°æ®å·²ä¿å­˜åˆ°: {output_path}")
            print(f"  æ–‡ä»¶å¤§å°: {self.format_size(os.path.getsize(output_path))}")
            return output_path
        except Exception as e:
            print(f"âœ— ä¿å­˜æ–‡ä»¶å¤±è´¥: {e}")
            return None


# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    print("=" * 80)
    print("DAQ åˆ†æå™¨ - ä½¿ç”¨ç¤ºä¾‹")
    print("=" * 80)
    print("\nåŸºæœ¬ç”¨æ³•ï¼š")
    print("  # åˆ›å»ºåˆ†æå™¨å¹¶æ‰«ææ‰€æœ‰è¿è¡Œ")
    print("  analyzer = DAQAnalyzer()")
    print("  analyzer.scan_all_runs()")
    print("")
    print("  # æ˜¾ç¤ºæ¦‚è§ˆå’Œç»Ÿè®¡")
    print("  analyzer.display_overview()")
    print("  analyzer.display_summary()")
    print("")
    print("  # æ˜¾ç¤ºæŒ‡å®šè¿è¡Œçš„é€šé“æ—¶é—´ä¿¡æ¯")
    print("  analyzer.display_run_channel_details('50V_OV_circulation_20thr')")
    print("")
    print("  # è·å–æŒ‡å®šè¿è¡Œçš„ DAQRun å¯¹è±¡è¿›è¡Œæ·±åº¦åˆ†æ")
    print("  run = analyzer.get_run('50V_OV_circulation_20thr')")
    print("  stats = run.compute_acquisition_times()")
    print("=" * 80)
